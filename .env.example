# ci-helper 環境変数設定例
# このファイルを .env にコピーして使用してください

# 基本設定（ci-helper.tomlの設定を上書き）
# CI_HELPER_VERBOSE=false
# CI_HELPER_SAVE_LOGS=true
# CI_HELPER_TIMEOUT_SECONDS=1800

# act実行設定
# CI_HELPER_ACT_IMAGE=ghcr.io/catthehacker/ubuntu:full-24.04

# ログ設定
# CI_HELPER_LOG_DIR=.ci-helper/logs
# CI_HELPER_CACHE_DIR=.ci-helper/cache
# CI_HELPER_MAX_LOG_SIZE_MB=100

# AI統合設定
# 重要: APIキーは機密情報です。このファイルを.gitignoreに追加してください

# AIプロバイダーのAPIキー（必須）
# OpenAI APIキー（https://platform.openai.com/api-keys で取得）
# OPENAI_API_KEY=sk-proj-your-openai-api-key-here

# Anthropic APIキー（https://console.anthropic.com/keys で取得）
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# ローカルLLM設定（Ollama使用時）
# OLLAMA_BASE_URL=http://localhost:11434

# AI設定の上書き（ci-helper.tomlの設定より優先される）
# CI_HELPER_AI_PROVIDER=openai                    # デフォルトプロバイダー
# CI_HELPER_AI_MODEL=gpt-4o                       # デフォルトモデル
# CI_HELPER_AI_CACHE_ENABLED=true                 # キャッシュ有効化
# CI_HELPER_AI_INTERACTIVE_TIMEOUT=300            # 対話タイムアウト（秒）

# プロバイダー別設定の上書き
# CI_HELPER_AI_PROVIDERS_OPENAI_DEFAULT_MODEL=gpt-4o-mini
# CI_HELPER_AI_PROVIDERS_ANTHROPIC_DEFAULT_MODEL=claude-3-5-haiku-20241022
# CI_HELPER_AI_PROVIDERS_LOCAL_DEFAULT_MODEL=llama3.2

# コスト制限の上書き
# CI_HELPER_AI_COST_LIMITS_MONTHLY_USD=25.0       # 月間制限を25ドルに
# CI_HELPER_AI_COST_LIMITS_PER_REQUEST_USD=0.5    # 1回あたり50セントに制限

# キャッシュ設定の上書き
# CI_HELPER_AI_CACHE_MAX_SIZE_MB=200              # キャッシュサイズを200MBに
# CI_HELPER_AI_CACHE_TTL_HOURS=48                 # キャッシュ有効期限を48時間に

# セキュリティ設定の上書き
# CI_HELPER_AI_SECURITY_MASK_SECRETS=true         # シークレットマスキング
# CI_HELPER_AI_SECURITY_VERIFY_SSL=true           # SSL検証

# パフォーマンス設定の上書き
# CI_HELPER_AI_PERFORMANCE_CONCURRENT_REQUESTS=1  # 並列リクエスト数を1に制限
# CI_HELPER_AI_PERFORMANCE_MEMORY_LIMIT_MB=256    # メモリ制限を256MBに

# デバッグ設定
# CI_HELPER_AI_DEBUG=true                         # AIデバッグモード
# CI_HELPER_AI_LOG_LEVEL=DEBUG                    # AIログレベル

# GitHub設定（必要に応じて）
# GITHUB_TOKEN=your_github_token_here
