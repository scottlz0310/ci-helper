"""
ãƒ­ã‚°è§£æã‚·ã‚¹ãƒ†ãƒ 

actã®å®Ÿè¡Œãƒ­ã‚°ã‚’è§£æã—ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã‚¸ãƒ§ãƒ–ã”ã¨ã«å¤±æ•—ã‚’æ•´ç†ã—ã¾ã™ã€‚
"""

from __future__ import annotations

import re
from datetime import datetime

from ..core.exceptions import LogParsingError
from ..core.log_extractor import LogExtractor
from ..core.models import ExecutionResult, Failure, JobResult, StepResult, WorkflowResult


class LogAnalyzer:
    """ãƒ­ã‚°ã‚’è§£æã—ã¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã‚¸ãƒ§ãƒ–ã”ã¨ã«å¤±æ•—ã‚’æ•´ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, log_extractor: LogExtractor | None = None):
        """ãƒ­ã‚°ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–

        Args:
            log_extractor: ãƒ­ã‚°æŠ½å‡ºå™¨ï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
        """
        self.log_extractor = log_extractor or LogExtractor()
        self._compile_act_patterns()

    def _compile_act_patterns(self) -> None:
        """actç‰¹æœ‰ã®ãƒ­ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«"""
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.workflow_start_pattern = re.compile(r"^\[.*\]\s*ğŸš€\s*Start image=.*$", re.MULTILINE)

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åæŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        self.workflow_name_pattern = re.compile(r"^\[.*\]\s*.*Job '([^']+)' is about to start", re.MULTILINE)

        # ã‚¸ãƒ§ãƒ–é–‹å§‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.job_start_pattern = re.compile(r"^\[.*\]\s*ğŸš€\s*Starting job: ([^\s]+)", re.MULTILINE)

        # ã‚¸ãƒ§ãƒ–çµ‚äº†ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.job_end_pattern = re.compile(r"^\[.*\]\s*âœ…\s*Job succeeded|^\[.*\]\s*âŒ\s*Job failed", re.MULTILINE)

        # ã‚¹ãƒ†ãƒƒãƒ—é–‹å§‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.step_start_pattern = re.compile(r"^\[.*\]\s*â­\s*Run (.+)$", re.MULTILINE)

        # ã‚¹ãƒ†ãƒƒãƒ—çµ‚äº†ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.step_end_pattern = re.compile(r"^\[.*\]\s*âœ…\s*Success - (.+)|^\[.*\]\s*âŒ\s*Failure - (.+)", re.MULTILINE)

        # å®Ÿè¡Œæ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.duration_pattern = re.compile(r"took (\d+(?:\.\d+)?)s", re.MULTILINE)

        # ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.error_output_pattern = re.compile(r"^\[.*\]\s*ğŸ’¬\s*(.+)$", re.MULTILINE)

    def analyze_log(self, log_content: str, workflows: list[str] | None = None) -> ExecutionResult:
        """ãƒ­ã‚°ã‚’è§£æã—ã¦ExecutionResultã‚’ç”Ÿæˆ

        Args:
            log_content: actã®å®Ÿè¡Œãƒ­ã‚°
            workflows: å®Ÿè¡Œã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ¤œå‡ºï¼‰

        Returns:
            è§£æã•ã‚ŒãŸExecutionResult

        Raises:
            LogParsingError: ãƒ­ã‚°è§£æã«å¤±æ•—ã—ãŸå ´åˆ
        """
        if not log_content or not log_content.strip():
            raise LogParsingError("ãƒ­ã‚°ãŒç©ºã§ã™", "æœ‰åŠ¹ãªactã®å®Ÿè¡Œãƒ­ã‚°ã‚’æä¾›ã—ã¦ãã ã•ã„")

        try:
            # å…¨ä½“ã®å¤±æ•—ã‚’æŠ½å‡º
            all_failures = self.log_extractor.extract_failures(log_content)

            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¤œå‡ºã¾ãŸã¯ä½¿ç”¨
            detected_workflows = workflows or self._detect_workflows(log_content)

            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã”ã¨ã«è§£æ
            workflow_results: list[WorkflowResult] = []
            total_duration = 0.0
            overall_success = True

            for workflow_name in detected_workflows:
                workflow_result = self._analyze_workflow(log_content, workflow_name, all_failures)
                workflow_results.append(workflow_result)
                total_duration += workflow_result.duration
                if not workflow_result.success:
                    overall_success = False

            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒæ¤œå‡ºã•ã‚Œãªã„å ´åˆã¯å˜ä¸€ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã—ã¦æ‰±ã†
            if not workflow_results:
                workflow_result = self._analyze_single_workflow(log_content, all_failures)
                workflow_results.append(workflow_result)
                total_duration = workflow_result.duration
                overall_success = workflow_result.success

            return ExecutionResult(
                success=overall_success,
                workflows=workflow_results,
                total_duration=total_duration,
            )

        except Exception as e:
            raise LogParsingError(
                f"ãƒ­ã‚°è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
            ) from e

    def _detect_workflows(self, log_content: str) -> list[str]:
        """ãƒ­ã‚°ã‹ã‚‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åã‚’æ¤œå‡º

        Args:
            log_content: ãƒ­ã‚°å†…å®¹

        Returns:
            æ¤œå‡ºã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åã®ãƒªã‚¹ãƒˆ
        """
        workflow_names: set[str] = set()

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æŠ½å‡º
        for match in self.workflow_name_pattern.finditer(log_content):
            workflow_name = match.group(1)
            workflow_names.add(workflow_name)

        # ã‚¸ãƒ§ãƒ–åã‹ã‚‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¨æ¸¬
        for match in self.job_start_pattern.finditer(log_content):
            job_name = match.group(1)
            # ã‚¸ãƒ§ãƒ–åã‹ã‚‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åã‚’æ¨æ¸¬ï¼ˆé€šå¸¸ã¯ãƒ•ã‚¡ã‚¤ãƒ«åãƒ™ãƒ¼ã‚¹ï¼‰
            if "/" in job_name:
                workflow_name = job_name.split("/")[0]
                workflow_names.add(workflow_name)

        return list(workflow_names)

    def _analyze_workflow(self, log_content: str, workflow_name: str, all_failures: list[Failure]) -> WorkflowResult:
        """ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’è§£æ

        Args:
            log_content: ãƒ­ã‚°å†…å®¹
            workflow_name: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å
            all_failures: å…¨å¤±æ•—ã®ãƒªã‚¹ãƒˆ

        Returns:
            ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è§£æçµæœ
        """
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–¢é€£ã®ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        workflow_section = self._extract_workflow_section(log_content, workflow_name)

        # ã‚¸ãƒ§ãƒ–ã‚’æ¤œå‡ºãƒ»è§£æ
        jobs = self._analyze_jobs(workflow_section, all_failures)

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æˆåŠŸ/å¤±æ•—ã‚’åˆ¤å®š
        workflow_success = all(job.success for job in jobs)

        # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
        workflow_duration = sum(job.duration for job in jobs)

        return WorkflowResult(
            name=workflow_name,
            success=workflow_success,
            jobs=jobs,
            duration=workflow_duration,
        )

    def _analyze_single_workflow(self, log_content: str, all_failures: list[Failure]) -> WorkflowResult:
        """å˜ä¸€ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã—ã¦ãƒ­ã‚°ã‚’è§£æ

        Args:
            log_content: ãƒ­ã‚°å†…å®¹
            all_failures: å…¨å¤±æ•—ã®ãƒªã‚¹ãƒˆ

        Returns:
            ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è§£æçµæœ
        """
        # ã‚¸ãƒ§ãƒ–ã‚’æ¤œå‡ºãƒ»è§£æ
        jobs = self._analyze_jobs(log_content, all_failures)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å
        workflow_name = "default"

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æˆåŠŸ/å¤±æ•—ã‚’åˆ¤å®š
        workflow_success = all(job.success for job in jobs) if jobs else True

        # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
        workflow_duration = sum(job.duration for job in jobs)

        return WorkflowResult(
            name=workflow_name,
            success=workflow_success,
            jobs=jobs,
            duration=workflow_duration,
        )

    def _extract_workflow_section(self, log_content: str, workflow_name: str) -> str:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–¢é€£ã®ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º

        Args:
            log_content: å…¨ãƒ­ã‚°å†…å®¹
            workflow_name: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å

        Returns:
            ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–¢é€£ã®ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        """
        lines = log_content.splitlines()
        workflow_lines: list[str] = []
        in_workflow = False

        for line in lines:
            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹ã®æ¤œå‡º
            if workflow_name in line and ("Starting" in line or "Start" in line):
                in_workflow = True

            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ‚äº†ã®æ¤œå‡º
            elif in_workflow and ("Job succeeded" in line or "Job failed" in line):
                workflow_lines.append(line)
                # æ¬¡ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒå§‹ã¾ã‚‹ã¾ã§ç¶™ç¶š
                continue

            if in_workflow:
                workflow_lines.append(line)

        return "\n".join(workflow_lines)

    def _analyze_jobs(self, log_section: str, all_failures: list[Failure]) -> list[JobResult]:
        """ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚¸ãƒ§ãƒ–ã‚’è§£æ

        Args:
            log_section: è§£æå¯¾è±¡ã®ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            all_failures: å…¨å¤±æ•—ã®ãƒªã‚¹ãƒˆ

        Returns:
            ã‚¸ãƒ§ãƒ–ã®è§£æçµæœãƒªã‚¹ãƒˆ
        """
        jobs: list[JobResult] = []

        # ã‚¸ãƒ§ãƒ–é–‹å§‹ä½ç½®ã‚’æ¤œå‡º
        job_matches: list[re.Match[str]] = list(self.job_start_pattern.finditer(log_section))

        if not job_matches:
            # ã‚¸ãƒ§ãƒ–ãŒæ˜ç¤ºçš„ã«æ¤œå‡ºã•ã‚Œãªã„å ´åˆã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¸ãƒ§ãƒ–ã‚’ä½œæˆ
            job_result = self._create_default_job(log_section, all_failures)
            if job_result:
                jobs.append(job_result)
        else:
            # å„ã‚¸ãƒ§ãƒ–ã‚’è§£æ
            for i, match in enumerate(job_matches):
                job_name = match.group(1)

                # ã‚¸ãƒ§ãƒ–ã®ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
                start_pos = match.start()
                end_pos = job_matches[i + 1].start() if i + 1 < len(job_matches) else len(log_section)
                job_section = log_section[start_pos:end_pos]

                # ã‚¸ãƒ§ãƒ–ã‚’è§£æ
                job_result = self._analyze_single_job(job_name, job_section, all_failures)
                jobs.append(job_result)

        return jobs

    def _create_default_job(self, log_section: str, all_failures: list[Failure]) -> JobResult | None:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¸ãƒ§ãƒ–ã‚’ä½œæˆ

        Args:
            log_section: ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            all_failures: å…¨å¤±æ•—ã®ãƒªã‚¹ãƒˆ

        Returns:
            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¸ãƒ§ãƒ–ã®çµæœï¼ˆä½œæˆã§ããªã„å ´åˆã¯Noneï¼‰
        """
        if not log_section.strip():
            return None

        # ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®å¤±æ•—ã‚’æŠ½å‡º
        job_failures = self._extract_failures_from_section(log_section, all_failures)

        # ã‚¹ãƒ†ãƒƒãƒ—ã‚’è§£æ
        steps = self._analyze_steps(log_section)

        # æˆåŠŸ/å¤±æ•—ã‚’åˆ¤å®š
        job_success = len(job_failures) == 0 and all(step.success for step in steps)

        # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
        job_duration = sum(step.duration for step in steps)
        if job_duration == 0:
            # ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰æ™‚é–“ãŒå–å¾—ã§ããªã„å ´åˆã¯ã€ãƒ­ã‚°ã‹ã‚‰æ¨æ¸¬
            job_duration = self._estimate_duration_from_log(log_section)

        return JobResult(
            name="default",
            success=job_success,
            failures=job_failures,
            steps=steps,
            duration=job_duration,
        )

    def _analyze_single_job(self, job_name: str, job_section: str, all_failures: list[Failure]) -> JobResult:
        """å˜ä¸€ã®ã‚¸ãƒ§ãƒ–ã‚’è§£æ

        Args:
            job_name: ã‚¸ãƒ§ãƒ–å
            job_section: ã‚¸ãƒ§ãƒ–ã®ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            all_failures: å…¨å¤±æ•—ã®ãƒªã‚¹ãƒˆ

        Returns:
            ã‚¸ãƒ§ãƒ–ã®è§£æçµæœ
        """
        # ã‚¸ãƒ§ãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®å¤±æ•—ã‚’æŠ½å‡º
        job_failures = self._extract_failures_from_section(job_section, all_failures)

        # ã‚¹ãƒ†ãƒƒãƒ—ã‚’è§£æ
        steps = self._analyze_steps(job_section)

        # æˆåŠŸ/å¤±æ•—ã‚’åˆ¤å®š
        job_success = len(job_failures) == 0 and all(step.success for step in steps)

        # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
        job_duration = sum(step.duration for step in steps)
        if job_duration == 0:
            job_duration = self._estimate_duration_from_log(job_section)

        return JobResult(
            name=job_name,
            success=job_success,
            failures=job_failures,
            steps=steps,
            duration=job_duration,
        )

    def _analyze_steps(self, job_section: str) -> list[StepResult]:
        """ã‚¸ãƒ§ãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚¹ãƒ†ãƒƒãƒ—ã‚’è§£æ

        Args:
            job_section: ã‚¸ãƒ§ãƒ–ã®ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³

        Returns:
            ã‚¹ãƒ†ãƒƒãƒ—ã®è§£æçµæœãƒªã‚¹ãƒˆ
        """
        steps: list[StepResult] = []

        # ã‚¹ãƒ†ãƒƒãƒ—é–‹å§‹ä½ç½®ã‚’æ¤œå‡º
        step_matches: list[re.Match[str]] = list(self.step_start_pattern.finditer(job_section))

        for i, match in enumerate(step_matches):
            step_name = match.group(1)

            # ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
            start_pos = match.start()
            end_pos = step_matches[i + 1].start() if i + 1 < len(step_matches) else len(job_section)
            step_section = job_section[start_pos:end_pos]

            # ã‚¹ãƒ†ãƒƒãƒ—ã®æˆåŠŸ/å¤±æ•—ã‚’åˆ¤å®š
            step_success = "âŒ" not in step_section and "Failure" not in step_section

            # ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œæ™‚é–“ã‚’æŠ½å‡º
            step_duration = self._extract_duration_from_section(step_section)

            # ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã‚’æŠ½å‡º
            step_output = self._extract_step_output(step_section)

            steps.append(
                StepResult(
                    name=step_name,
                    success=step_success,
                    duration=step_duration,
                    output=step_output,
                )
            )

        return steps

    def _extract_failures_from_section(self, section: str, all_failures: list[Failure]) -> list[Failure]:
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®å¤±æ•—ã‚’æŠ½å‡º

        Args:
            section: ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            all_failures: å…¨å¤±æ•—ã®ãƒªã‚¹ãƒˆ

        Returns:
            ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®å¤±æ•—ã®ãƒªã‚¹ãƒˆ
        """
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å›ºæœ‰ã®å¤±æ•—ã‚’ç›´æ¥æŠ½å‡ºï¼ˆã‚ˆã‚Šæ­£ç¢ºï¼‰
        section_failures: list[Failure] = self.log_extractor.extract_failures(section)

        # å…¨å¤±æ•—ãƒªã‚¹ãƒˆã‹ã‚‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®å¤±æ•—ã‚‚ç¢ºèª
        for failure in all_failures:
            # å¤±æ•—ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if failure.message in section:
                # æ—¢ã«å«ã¾ã‚Œã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ 
                if not any(
                    f.message == failure.message
                    and f.file_path == failure.file_path
                    and f.line_number == failure.line_number
                    for f in section_failures
                ):
                    section_failures.append(failure)

        return self._deduplicate_failures(section_failures)

    def _extract_duration_from_section(self, section: str) -> float:
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å®Ÿè¡Œæ™‚é–“ã‚’æŠ½å‡º

        Args:
            section: ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³

        Returns:
            å®Ÿè¡Œæ™‚é–“ï¼ˆç§’ï¼‰
        """
        duration_match = self.duration_pattern.search(section)
        if duration_match:
            try:
                return float(duration_match.group(1))
            except ValueError:
                pass

        return 0.0

    def _estimate_duration_from_log(self, log_section: str) -> float:
        """ãƒ­ã‚°ã‹ã‚‰å®Ÿè¡Œæ™‚é–“ã‚’æ¨æ¸¬

        Args:
            log_section: ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³

        Returns:
            æ¨å®šå®Ÿè¡Œæ™‚é–“ï¼ˆç§’ï¼‰
        """
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ™‚é–“ã‚’æ¨æ¸¬
        timestamp_pattern = re.compile(r"\[(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})")
        timestamps: list[datetime] = []

        for match in timestamp_pattern.finditer(log_section):
            timestamp_str = match.group(1)
            try:
                from datetime import datetime

                timestamp = datetime.fromisoformat(timestamp_str)
                timestamps.append(timestamp)
            except ValueError:
                continue

        if len(timestamps) >= 2:
            duration = (timestamps[-1] - timestamps[0]).total_seconds()
            return max(0.0, duration)

        return 0.0

    def _extract_step_output(self, step_section: str) -> str:
        """ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã‚’æŠ½å‡º

        Args:
            step_section: ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³

        Returns:
            ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›
        """
        output_lines: list[str] = []

        # ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å‡ºåŠ›ã‚’æŠ½å‡º
        for match in self.error_output_pattern.finditer(step_section):
            output_lines.append(match.group(1))

        # é€šå¸¸ã®å‡ºåŠ›è¡Œã‚‚å«ã‚ã‚‹ï¼ˆactã®å‡ºåŠ›å½¢å¼ã«ä¾å­˜ï¼‰
        lines = step_section.splitlines()
        for line in lines:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®å‡ºåŠ›è¡Œã‚’æŠ½å‡º
            if re.match(r"^\[.*\]\s*ğŸ’¬", line):
                continue  # æ—¢ã«ä¸Šã§å‡¦ç†æ¸ˆã¿
            elif re.match(r"^\[.*\]\s*", line):
                # ãã®ä»–ã®actå‡ºåŠ›
                clean_line = re.sub(r"^\[.*\]\s*", "", line)
                if clean_line.strip():
                    output_lines.append(clean_line)

        return "\n".join(output_lines)

    def compare_execution_results(
        self, current: ExecutionResult, previous: ExecutionResult
    ) -> dict[str, list[Failure]]:
        """å®Ÿè¡Œçµæœã‚’æ¯”è¼ƒã—ã¦å·®åˆ†ã‚’æŠ½å‡º

        Args:
            current: ç¾åœ¨ã®å®Ÿè¡Œçµæœ
            previous: å‰å›ã®å®Ÿè¡Œçµæœ

        Returns:
            å·®åˆ†æƒ…å ±ï¼ˆnew_errors, resolved_errors, persistent_errorsï¼‰
        """
        # ç¾åœ¨ã¨å‰å›ã®å¤±æ•—ã‚’åé›†
        current_failures = self._collect_all_failures(current)
        previous_failures = self._collect_all_failures(previous)

        # å¤±æ•—ã‚’æ¯”è¼ƒç”¨ã®ã‚­ãƒ¼ã§åˆ†é¡
        current_keys = {self._failure_key(f): f for f in current_failures}
        previous_keys = {self._failure_key(f): f for f in previous_failures}

        # å·®åˆ†ã‚’è¨ˆç®—
        new_errors = [current_keys[key] for key in current_keys if key not in previous_keys]

        resolved_errors = [previous_keys[key] for key in previous_keys if key not in current_keys]

        persistent_errors = [current_keys[key] for key in current_keys if key in previous_keys]

        return {
            "new_errors": new_errors,
            "resolved_errors": resolved_errors,
            "persistent_errors": persistent_errors,
        }

    def _collect_all_failures(self, execution_result: ExecutionResult) -> list[Failure]:
        """ExecutionResultã‹ã‚‰å…¨ã¦ã®å¤±æ•—ã‚’åé›†

        Args:
            execution_result: å®Ÿè¡Œçµæœ

        Returns:
            å…¨å¤±æ•—ã®ãƒªã‚¹ãƒˆ
        """
        all_failures: list[Failure] = []
        for workflow in execution_result.workflows:
            for job in workflow.jobs:
                all_failures.extend(job.failures)
        return all_failures

    def _failure_key(self, failure: Failure) -> tuple[str, str | None, int | None]:
        """å¤±æ•—ã®æ¯”è¼ƒç”¨ã‚­ãƒ¼ã‚’ç”Ÿæˆ

        Args:
            failure: å¤±æ•—ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

        Returns:
            æ¯”è¼ƒç”¨ã®ã‚­ãƒ¼
        """
        return (failure.message, failure.file_path, failure.line_number)

    def _deduplicate_failures(self, failures: list[Failure]) -> list[Failure]:
        """é‡è¤‡ã™ã‚‹å¤±æ•—ã‚’é™¤å»

        Args:
            failures: å¤±æ•—ã®ãƒªã‚¹ãƒˆ

        Returns:
            é‡è¤‡ã‚’é™¤å»ã—ãŸå¤±æ•—ã®ãƒªã‚¹ãƒˆ
        """
        seen: set[tuple[str, str | None, int | None]] = set()
        unique_failures: list[Failure] = []

        for failure in failures:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€è¡Œç•ªå·ã®çµ„ã¿åˆã‚ã›ã§é‡è¤‡åˆ¤å®š
            key = self._failure_key(failure)
            if key not in seen:
                seen.add(key)
                unique_failures.append(failure)

        return unique_failures
