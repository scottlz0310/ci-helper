"""
AIç”¨å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼

CIå®Ÿè¡Œçµæœã‚’AIæ¶ˆè²»ç”¨ã®MarkdownãŠã‚ˆã³JSONå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™ã€‚
"""

from __future__ import annotations

import json
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    pass

try:
    import tiktoken
except ImportError:
    tiktoken = None

from ..core.models import AnalysisMetrics, ExecutionResult, Failure, FailureType, JobResult, WorkflowResult
from ..core.security import SecurityValidator


class AIFormatter:
    """AIæ¶ˆè²»ç”¨ã®å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼"""

    def __init__(self, sanitize_secrets: bool = True):
        """ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’åˆæœŸåŒ–

        Args:
            sanitize_secrets: ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã®ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹
        """
        self.failure_type_icons = {
            FailureType.ERROR: "ğŸš¨",
            FailureType.ASSERTION: "âŒ",
            FailureType.TIMEOUT: "â°",
            FailureType.BUILD_FAILURE: "ğŸ”¨",
            FailureType.TEST_FAILURE: "ğŸ§ª",
            FailureType.UNKNOWN: "â“",
        }
        self.sanitize_secrets = sanitize_secrets
        if sanitize_secrets:
            self.security_validator = SecurityValidator()

    def format_markdown(self, execution_result: ExecutionResult) -> str:
        """å®Ÿè¡Œçµæœã‚’Markdownå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

        Args:
            execution_result: CIå®Ÿè¡Œçµæœ

        Returns:
            Markdownå½¢å¼ã®æ–‡å­—åˆ—
        """
        sections = []

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        sections.append(self._format_markdown_header(execution_result))

        # å®Ÿè¡Œã‚µãƒãƒªãƒ¼
        sections.append(self._format_markdown_summary(execution_result))

        # å¤±æ•—ãŒã‚ã‚‹å ´åˆã®è©³ç´°
        if not execution_result.success:
            sections.append(self._format_markdown_failures(execution_result))

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è©³ç´°
        sections.append(self._format_markdown_workflows(execution_result))

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        metrics = AnalysisMetrics.from_execution_result(execution_result)
        sections.append(self._format_markdown_metrics(metrics))

        markdown_content = "\n\n".join(sections)

        # ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        if self.sanitize_secrets:
            markdown_content = self._sanitize_content(markdown_content)

        return markdown_content

    def _format_markdown_header(self, execution_result: ExecutionResult) -> str:
        """Markdownãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
        status_icon = "âœ…" if execution_result.success else "âŒ"
        status_text = "æˆåŠŸ" if execution_result.success else "å¤±æ•—"

        return f"""# CIå®Ÿè¡Œçµæœ {status_icon}

**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: {status_text}
**å®Ÿè¡Œæ™‚åˆ»**: {execution_result.timestamp.strftime("%Y-%m-%d %H:%M:%S")}
**ç·å®Ÿè¡Œæ™‚é–“**: {execution_result.total_duration:.2f}ç§’
**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ•°**: {len(execution_result.workflows)}"""

    def _format_markdown_summary(self, execution_result: ExecutionResult) -> str:
        """å®Ÿè¡Œã‚µãƒãƒªãƒ¼ã‚’Markdownå½¢å¼ã§ç”Ÿæˆ"""
        total_jobs = sum(len(w.jobs) for w in execution_result.workflows)
        successful_jobs = sum(1 for w in execution_result.workflows for j in w.jobs if j.success)
        total_failures = execution_result.total_failures

        summary = f"""## ğŸ“Š å®Ÿè¡Œã‚µãƒãƒªãƒ¼

- **ç·ã‚¸ãƒ§ãƒ–æ•°**: {total_jobs}
- **æˆåŠŸã‚¸ãƒ§ãƒ–**: {successful_jobs}
- **å¤±æ•—ã‚¸ãƒ§ãƒ–**: {total_jobs - successful_jobs}
- **ç·å¤±æ•—æ•°**: {total_failures}
- **æˆåŠŸç‡**: {(successful_jobs / total_jobs * 100) if total_jobs > 0 else 100:.1f}%"""

        return summary

    def _format_markdown_failures(self, execution_result: ExecutionResult) -> str:
        """å¤±æ•—è©³ç´°ã‚’Markdownå½¢å¼ã§ç”Ÿæˆ"""
        if execution_result.success:
            return ""

        sections = ["## ğŸš¨ å¤±æ•—è©³ç´°"]

        # å¤±æ•—ã‚¿ã‚¤ãƒ—åˆ¥ã®é›†è¨ˆ
        failure_counts = {}
        for failure in execution_result.all_failures:
            failure_counts[failure.type] = failure_counts.get(failure.type, 0) + 1

        if failure_counts:
            sections.append("### å¤±æ•—ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ")
            for failure_type, count in failure_counts.items():
                icon = self.failure_type_icons.get(failure_type, "â“")
                sections.append(f"- {icon} **{failure_type.value}**: {count}ä»¶")

        # å„å¤±æ•—ã®è©³ç´°
        sections.append("### å¤±æ•—ä¸€è¦§")

        failure_num = 1
        for workflow in execution_result.workflows:
            if not workflow.success:
                for job in workflow.jobs:
                    if not job.success:
                        for failure in job.failures:
                            sections.append(
                                self._format_single_failure_markdown(failure, failure_num, workflow.name, job.name)
                            )
                            failure_num += 1

        return "\n\n".join(sections)

    def _format_single_failure_markdown(
        self, failure: Failure, failure_num: int, workflow_name: str, job_name: str
    ) -> str:
        """å˜ä¸€ã®å¤±æ•—ã‚’Markdownå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        icon = self.failure_type_icons.get(failure.type, "â“")

        sections = [f"#### {failure_num}. {icon} {failure.type.value.upper()}"]

        # åŸºæœ¬æƒ…å ±
        sections.append(f"**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**: {workflow_name}")
        sections.append(f"**ã‚¸ãƒ§ãƒ–**: {job_name}")

        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        if failure.file_path:
            file_info = f"**ãƒ•ã‚¡ã‚¤ãƒ«**: `{failure.file_path}`"
            if failure.line_number:
                file_info += f" (è¡Œ {failure.line_number})"
            sections.append(file_info)

        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        sections.append("**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**:")
        sections.append(f"```\n{failure.message}\n```")

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå‰å¾Œã®è¡Œï¼‰
        if failure.context_before or failure.context_after:
            sections.append("**ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**:")
            context_lines = []

            # å‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            for line in failure.context_before:
                context_lines.append(f"  {line}")

            # ã‚¨ãƒ©ãƒ¼è¡Œï¼ˆæ¨å®šï¼‰
            context_lines.append(f"> {failure.message}")

            # å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            for line in failure.context_after:
                context_lines.append(f"  {line}")

            sections.append("```\n" + "\n".join(context_lines) + "\n```")

        # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹
        if failure.stack_trace:
            sections.append("**ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹**:")
            sections.append(f"```\n{failure.stack_trace}\n```")

        return "\n".join(sections)

    def _format_markdown_workflows(self, execution_result: ExecutionResult) -> str:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è©³ç´°ã‚’Markdownå½¢å¼ã§ç”Ÿæˆ"""
        sections = ["## ğŸ“‹ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è©³ç´°"]

        for workflow in execution_result.workflows:
            workflow_icon = "âœ…" if workflow.success else "âŒ"
            sections.append(f"### {workflow_icon} {workflow.name}")

            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æƒ…å ±
            sections.append(f"- **å®Ÿè¡Œæ™‚é–“**: {workflow.duration:.2f}ç§’")
            sections.append(f"- **ã‚¸ãƒ§ãƒ–æ•°**: {len(workflow.jobs)}")
            sections.append(f"- **æˆåŠŸã‚¸ãƒ§ãƒ–**: {sum(1 for j in workflow.jobs if j.success)}")

            # ã‚¸ãƒ§ãƒ–è©³ç´°
            if workflow.jobs:
                sections.append("#### ã‚¸ãƒ§ãƒ–ä¸€è¦§")
                for job in workflow.jobs:
                    job_icon = "âœ…" if job.success else "âŒ"
                    failure_count = len(job.failures)
                    failure_text = f" ({failure_count}ä»¶ã®å¤±æ•—)" if failure_count > 0 else ""
                    sections.append(f"- {job_icon} **{job.name}** - {job.duration:.2f}ç§’{failure_text}")

        return "\n\n".join(sections)

    def _format_markdown_metrics(self, metrics: AnalysisMetrics) -> str:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’Markdownå½¢å¼ã§ç”Ÿæˆ"""
        sections = ["## ğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹"]

        sections.append(f"- **ç·ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ•°**: {metrics.total_workflows}")
        sections.append(f"- **ç·ã‚¸ãƒ§ãƒ–æ•°**: {metrics.total_jobs}")
        sections.append(f"- **ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°**: {metrics.total_steps}")
        sections.append(f"- **ç·å¤±æ•—æ•°**: {metrics.total_failures}")
        sections.append(f"- **æˆåŠŸç‡**: {metrics.success_rate:.1f}%")
        sections.append(f"- **å¹³å‡å®Ÿè¡Œæ™‚é–“**: {metrics.average_duration:.2f}ç§’")

        if metrics.failure_types:
            sections.append("\n### å¤±æ•—ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ")
            for failure_type, count in metrics.failure_types.items():
                icon = self.failure_type_icons.get(failure_type, "â“")
                sections.append(f"- {icon} **{failure_type.value}**: {count}ä»¶")

        return "\n".join(sections)

    def format_json(self, execution_result: ExecutionResult) -> str:
        """å®Ÿè¡Œçµæœã‚’JSONå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

        Args:
            execution_result: CIå®Ÿè¡Œçµæœ

        Returns:
            JSONå½¢å¼ã®æ–‡å­—åˆ—
        """
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç”Ÿæˆ
        metrics = AnalysisMetrics.from_execution_result(execution_result)

        # JSONæ§‹é€ ã‚’æ§‹ç¯‰
        json_data = {
            "execution_summary": {
                "success": execution_result.success,
                "timestamp": execution_result.timestamp.isoformat(),
                "total_duration": execution_result.total_duration,
                "total_workflows": len(execution_result.workflows),
                "total_failures": execution_result.total_failures,
            },
            "metrics": {
                "total_workflows": metrics.total_workflows,
                "total_jobs": metrics.total_jobs,
                "total_steps": metrics.total_steps,
                "total_failures": metrics.total_failures,
                "success_rate": metrics.success_rate,
                "average_duration": metrics.average_duration,
                "failure_types": {ft.value: count for ft, count in metrics.failure_types.items()},
            },
            "workflows": [self._workflow_to_dict(workflow) for workflow in execution_result.workflows],
            "failures": [self._failure_to_dict(failure) for failure in execution_result.all_failures],
        }

        json_content = json.dumps(json_data, indent=2, ensure_ascii=False)

        # ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        if self.sanitize_secrets:
            json_content = self._sanitize_content(json_content)

        return json_content

    def _workflow_to_dict(self, workflow: WorkflowResult) -> dict[str, Any]:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’dictå½¢å¼ã«å¤‰æ›"""
        return {
            "name": workflow.name,
            "success": workflow.success,
            "duration": workflow.duration,
            "jobs": [self._job_to_dict(job) for job in workflow.jobs],
        }

    def _job_to_dict(self, job: JobResult) -> dict[str, Any]:
        """ã‚¸ãƒ§ãƒ–ã‚’dictå½¢å¼ã«å¤‰æ›"""
        return {
            "name": job.name,
            "success": job.success,
            "duration": job.duration,
            "failure_count": len(job.failures),
            "failures": [self._failure_to_dict(failure) for failure in job.failures],
            "steps": [self._step_to_dict(step) for step in job.steps],
        }

    def _step_to_dict(self, step) -> dict[str, Any]:
        """ã‚¹ãƒ†ãƒƒãƒ—ã‚’dictå½¢å¼ã«å¤‰æ›"""
        return {
            "name": step.name,
            "success": step.success,
            "duration": step.duration,
            "output": step.output,
        }

    def _failure_to_dict(self, failure: Failure) -> dict[str, Any]:
        """å¤±æ•—ã‚’dictå½¢å¼ã«å¤‰æ›"""
        return {
            "type": failure.type.value,
            "message": failure.message,
            "file_path": failure.file_path,
            "line_number": failure.line_number,
            "context_before": list(failure.context_before),
            "context_after": list(failure.context_after),
            "stack_trace": failure.stack_trace,
        }

    def count_tokens(self, content: str, model: str = "gpt-4") -> int:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ

        Args:
            content: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            model: å¯¾è±¡ã®AIãƒ¢ãƒ‡ãƒ«å

        Returns:
            æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°

        Raises:
            ImportError: tiktokenãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆ
        """
        if tiktoken is None:
            raise ImportError(
                "tiktokenãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
                "pip install tiktoken ã¾ãŸã¯ uv add tiktoken ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚"
            )

        try:
            # ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            # æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯cl100k_baseã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
            encoding = tiktoken.get_encoding("cl100k_base")

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        tokens = encoding.encode(content)
        return len(tokens)

    def check_token_limits(self, content: str, model: str = "gpt-4") -> dict[str, Any]:
        """ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€è­¦å‘Šæƒ…å ±ã‚’è¿”ã™

        Args:
            content: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            model: å¯¾è±¡ã®AIãƒ¢ãƒ‡ãƒ«å

        Returns:
            ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã¨è­¦å‘Šã‚’å«ã‚€è¾æ›¸
        """
        # ãƒ¢ãƒ‡ãƒ«åˆ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™
        model_limits = {
            "gpt-4": 8192,
            "gpt-4-32k": 32768,
            "gpt-4-turbo": 128000,
            "gpt-4o": 128000,
            "gpt-3.5-turbo": 4096,
            "gpt-3.5-turbo-16k": 16384,
            "claude-3-haiku": 200000,
            "claude-3-sonnet": 200000,
            "claude-3-opus": 200000,
        }

        try:
            token_count = self.count_tokens(content, model)
        except ImportError:
            # tiktokenãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã§æ¨å®š
            token_count = len(content) // 4  # å¤§ã¾ã‹ãªæ¨å®šï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³â‰ˆ4æ–‡å­—ï¼‰

        # ãƒ¢ãƒ‡ãƒ«ã®åˆ¶é™ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯8192ï¼‰
        limit = model_limits.get(model, 8192)

        # ä½¿ç”¨ç‡ã‚’è¨ˆç®—
        usage_ratio = token_count / limit

        # è­¦å‘Šãƒ¬ãƒ™ãƒ«ã‚’æ±ºå®š
        warning_level = "none"
        warning_message = ""

        if usage_ratio >= 0.9:
            warning_level = "critical"
            warning_message = "ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒåˆ¶é™ã®90%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åœ§ç¸®ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
        elif usage_ratio >= 0.7:
            warning_level = "warning"
            warning_message = "ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒåˆ¶é™ã®70%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚"
        elif usage_ratio >= 0.5:
            warning_level = "info"
            warning_message = "ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒåˆ¶é™ã®50%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚"

        return {
            "token_count": token_count,
            "token_limit": limit,
            "usage_ratio": usage_ratio,
            "usage_percentage": usage_ratio * 100,
            "warning_level": warning_level,
            "warning_message": warning_message,
            "model": model,
        }

    def format_with_token_info(
        self, execution_result: ExecutionResult, format_type: str = "markdown", model: str = "gpt-4"
    ) -> dict[str, Any]:
        """ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµæœã¨ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã™

        Args:
            execution_result: CIå®Ÿè¡Œçµæœ
            format_type: å‡ºåŠ›å½¢å¼ï¼ˆ"markdown" ã¾ãŸã¯ "json"ï¼‰
            model: å¯¾è±¡ã®AIãƒ¢ãƒ‡ãƒ«å

        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµæœã¨ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’å«ã‚€è¾æ›¸
        """
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå®Ÿè¡Œ
        if format_type.lower() == "json":
            formatted_content = self.format_json(execution_result)
        else:
            formatted_content = self.format_markdown(execution_result)

        # ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’å–å¾—
        token_info = self.check_token_limits(formatted_content, model)

        return {
            "content": formatted_content,
            "format": format_type,
            "token_info": token_info,
        }

    def suggest_compression_options(self, execution_result: ExecutionResult) -> list[str]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åœ§ç¸®ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆ

        Args:
            execution_result: CIå®Ÿè¡Œçµæœ

        Returns:
            åœ§ç¸®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        suggestions = []

        # å¤±æ•—æ•°ãŒå¤šã„å ´åˆ
        if execution_result.total_failures > 10:
            suggestions.append("å¤±æ•—æ•°ãŒå¤šã„ãŸã‚ã€æœ€ã‚‚é‡è¦ãªå¤±æ•—ã®ã¿ã«çµã‚Šè¾¼ã‚€")

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡ŒãŒå¤šã„å ´åˆ
        has_long_context = Any(
            len(failure.context_before) + len(failure.context_after) > 6 for failure in execution_result.all_failures
        )
        if has_long_context:
            suggestions.append("ã‚¨ãƒ©ãƒ¼ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œæ•°ã‚’å‰Šæ¸›ã™ã‚‹")

        # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ãŒå¤šã„å ´åˆ
        has_stack_traces = Any(failure.stack_trace for failure in execution_result.all_failures)
        if has_stack_traces:
            suggestions.append("ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¦ç´„ã¾ãŸã¯é™¤å¤–ã™ã‚‹")

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ•°ãŒå¤šã„å ´åˆ
        if len(execution_result.workflows) > 5:
            suggestions.append("å¤±æ•—ã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ã¿ã«çµã‚Šè¾¼ã‚€")

        # ã‚¸ãƒ§ãƒ–æ•°ãŒå¤šã„å ´åˆ
        total_jobs = sum(len(w.jobs) for w in execution_result.workflows)
        if total_jobs > 10:
            suggestions.append("å¤±æ•—ã—ãŸã‚¸ãƒ§ãƒ–ã®ã¿ã«çµã‚Šè¾¼ã‚€")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ææ¡ˆ
        if not suggestions:
            suggestions.extend(
                [
                    "JSONå½¢å¼ã‚’ä½¿ç”¨ã—ã¦ã‚ˆã‚Šç°¡æ½”ãªå‡ºåŠ›ã«ã™ã‚‹",
                    "æˆåŠŸã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è©³ç´°ã‚’é™¤å¤–ã™ã‚‹",
                    "ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®ã¿ã«çµã‚Šè¾¼ã‚€",
                ]
            )

        return suggestions

    def _sanitize_content(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º

        Args:
            content: ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¯¾è±¡ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„

        Returns:
            ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        """
        if not self.sanitize_secrets or not hasattr(self, "security_validator"):
            return content

        try:
            return self.security_validator.secret_detector.sanitize_content(content)
        except Exception:
            # ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿”ã™
            return content

    def validate_output_security(self, content: str) -> dict[str, Any]:
        """å‡ºåŠ›ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’æ¤œè¨¼

        Args:
            content: æ¤œè¨¼å¯¾è±¡ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„

        Returns:
            ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼çµæœ
        """
        if not hasattr(self, "security_validator"):
            return {
                "has_secrets": False,
                "secret_count": 0,
                "detected_secrets": [],
                "recommendations": ["ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼ãŒç„¡åŠ¹ã«ãªã£ã¦ã„ã¾ã™"],
            }

        return self.security_validator.validate_log_content(content)
