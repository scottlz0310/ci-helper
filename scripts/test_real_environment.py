#!/usr/bin/env python3
"""
å®Ÿç’°å¢ƒã§ã®å‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ

AIçµ±åˆæ©Ÿèƒ½ã®å®Ÿéš›ã®APIã‚­ãƒ¼ã‚’ä½¿ç”¨ã—ãŸE2Eå‹•ä½œç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚
å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆOpenAIã€Anthropicã€ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼‰ã§ã®å‹•ä½œç¢ºèªã€
å¤§ããªãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã€
ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªã§ã®å¾©æ—§å‹•ä½œç¢ºèªã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
    uv run python scripts/test_real_environment.py [--provider PROVIDER] [--verbose]

ç’°å¢ƒå¤‰æ•°:
    OPENAI_API_KEY: OpenAI APIã‚­ãƒ¼
    ANTHROPIC_API_KEY: Anthropic APIã‚­ãƒ¼
    OLLAMA_BASE_URL: ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ãƒ™ãƒ¼ã‚¹URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
"""

import asyncio
import json
import os
import sys
import time
from pathlib import Path
from typing import Any

import click
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from ci_helper.ai.integration import AIIntegration
from ci_helper.ai.models import AIConfig, AnalyzeOptions, ProviderConfig

console = Console()


class RealEnvironmentTester:
    """å®Ÿç’°å¢ƒãƒ†ã‚¹ã‚¿ãƒ¼"""

    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.results: dict[str, Any] = {
            "provider_tests": {},
            "performance_tests": {},
            "error_recovery_tests": {},
            "summary": {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "start_time": None,
                "end_time": None,
            },
        }
        self.test_log_content = self._generate_test_log_content()

    def _generate_test_log_content(self) -> str:
        """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ­ã‚°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
        return """
=== CI/CD Test Failure Log ===

Run pytest tests/unit/test_example.py
FAILED tests/unit/test_example.py::test_calculation - AssertionError: Expected 4, got 5

================================ FAILURES =================================
_______________________________ test_calculation _______________________________

    def test_calculation():
        result = add_numbers(2, 2)
>       assert result == 4, f"Expected 4, got {result}"
E       AssertionError: Expected 4, got 5

tests/unit/test_example.py:15: AssertionError

=========================== short test summary info ============================
FAILED tests/unit/test_example.py::test_calculation - AssertionError: Expected 4, got 5
=========================== 1 failed, 0 passed in 0.12s ===========================

Build failed with exit code 1
"""

    def _generate_large_log_content(self, size_mb: float = 1.0) -> str:
        """å¤§ããªãƒ­ã‚°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
        base_content = self.test_log_content
        target_size = int(size_mb * 1024 * 1024)  # MB to bytes

        # ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç¹°ã‚Šè¿”ã—ã¦ç›®æ¨™ã‚µã‚¤ã‚ºã«åˆ°é”
        repeat_count = max(1, target_size // len(base_content))
        large_content = base_content * repeat_count

        # è¿½åŠ ã®ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’ç”Ÿæˆ
        additional_entries = []
        for i in range(100):
            additional_entries.append(f"""
ERROR: Test case {i} failed with timeout
  File "test_{i}.py", line {i * 10}, in test_function_{i}
    assert response.status_code == 200
  AssertionError: Expected 200, got 500
""")

        return large_content + "\n".join(additional_entries)

    async def run_all_tests(self, specific_provider: str | None = None) -> dict[str, Any]:
        """å…¨ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        console.print(Panel.fit("ğŸ§ª å®Ÿç’°å¢ƒã§ã®å‹•ä½œç¢ºèªã‚’é–‹å§‹", style="blue"))

        self.results["summary"]["start_time"] = time.time()

        try:
            # 1. ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ
            if specific_provider:
                await self._test_specific_provider(specific_provider)
            else:
                await self._test_all_providers()

            # 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            await self._test_performance()

            # 3. ã‚¨ãƒ©ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆ
            await self._test_error_recovery()

        except Exception as e:
            console.print(f"[red]ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}[/red]")
            if self.verbose:
                console.print_exception()

        finally:
            self.results["summary"]["end_time"] = time.time()
            self._generate_summary_report()

        return self.results

    async def _test_all_providers(self) -> None:
        """å…¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
        console.print("\n[bold blue]1. ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ[/bold blue]")

        providers_to_test = []

        # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç¢ºèª
        if os.getenv("OPENAI_API_KEY"):
            providers_to_test.append("openai")
        if os.getenv("ANTHROPIC_API_KEY"):
            providers_to_test.append("anthropic")
        if os.getenv("OLLAMA_BASE_URL") or self._check_local_ollama():
            providers_to_test.append("local")

        if not providers_to_test:
            console.print("[yellow]âš ï¸  åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰[/yellow]")
            console.print("[blue]ğŸ’¡ ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„:[/blue]")
            console.print("  â€¢ OPENAI_API_KEY=your_openai_key")
            console.print("  â€¢ ANTHROPIC_API_KEY=your_anthropic_key")
            console.print("  â€¢ OLLAMA_BASE_URL=http://localhost:11434 (ãƒ­ãƒ¼ã‚«ãƒ«LLMç”¨)")
            return

        for provider in providers_to_test:
            await self._test_specific_provider(provider)

    async def _test_specific_provider(self, provider: str) -> None:
        """ç‰¹å®šãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
        console.print(f"\n[cyan]ğŸ“¡ {provider.upper()} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ[/cyan]")

        test_result = {
            "provider": provider,
            "tests": {},
            "overall_status": "unknown",
            "error_message": None,
            "performance_metrics": {},
        }

        try:
            # AIçµ±åˆã‚’åˆæœŸåŒ–
            ai_integration = await self._create_ai_integration(provider)

            # åŸºæœ¬åˆ†æãƒ†ã‚¹ãƒˆ
            await self._test_basic_analysis(ai_integration, provider, test_result)

            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
            await self._test_streaming_analysis(ai_integration, provider, test_result)

            # ä¿®æ­£ææ¡ˆãƒ†ã‚¹ãƒˆ
            await self._test_fix_suggestions(ai_integration, provider, test_result)

            # å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ
            await self._test_interactive_mode(ai_integration, provider, test_result)

            # å…¨ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ãŸå ´åˆ
            test_result["overall_status"] = "passed"
            console.print(f"[green]âœ… {provider.upper()} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆå®Œäº†[/green]")

        except Exception as e:
            test_result["overall_status"] = "failed"
            test_result["error_message"] = str(e)
            console.print(f"[red]âŒ {provider.upper()} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}[/red]")
            if self.verbose:
                console.print_exception()

        self.results["provider_tests"][provider] = test_result
        self._update_test_counts(test_result["overall_status"])

    async def _test_basic_analysis(self, ai_integration: AIIntegration, provider: str, test_result: dict) -> None:
        """åŸºæœ¬åˆ†æãƒ†ã‚¹ãƒˆ"""
        console.print("  ğŸ” åŸºæœ¬åˆ†æãƒ†ã‚¹ãƒˆ...")

        start_time = time.time()

        try:
            options = AnalyzeOptions(
                provider=provider,
                model=None,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                custom_prompt=None,
                streaming=False,
                use_cache=True,
                generate_fixes=False,
            )

            result = await ai_integration.analyze_log(self.test_log_content, options)

            # çµæœã®æ¤œè¨¼
            assert result is not None, "åˆ†æçµæœãŒNone"
            assert result.summary, "è¦ç´„ãŒç©º"
            assert result.confidence_score >= 0.0, "ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãŒç„¡åŠ¹"

            analysis_time = time.time() - start_time

            test_result["tests"]["basic_analysis"] = {
                "status": "passed",
                "analysis_time": analysis_time,
                "summary_length": len(result.summary),
                "confidence_score": result.confidence_score,
                "tokens_used": result.tokens_used.total_tokens if result.tokens_used else 0,
            }

            console.print(f"    âœ… åŸºæœ¬åˆ†ææˆåŠŸ ({analysis_time:.2f}ç§’)")

        except Exception as e:
            test_result["tests"]["basic_analysis"] = {"status": "failed", "error": str(e)}
            console.print(f"    âŒ åŸºæœ¬åˆ†æå¤±æ•—: {e}")
            raise

    async def _test_streaming_analysis(self, ai_integration: AIIntegration, provider: str, test_result: dict) -> None:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åˆ†æãƒ†ã‚¹ãƒˆ"""
        console.print("  ğŸŒŠ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åˆ†æãƒ†ã‚¹ãƒˆ...")

        start_time = time.time()

        try:
            options = AnalyzeOptions(
                provider=provider,
                streaming=True,
                use_cache=False,  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹
            )

            chunks_received = 0
            total_content = ""

            async for chunk in ai_integration.stream_analyze(self.test_log_content, options):
                chunks_received += 1
                total_content += chunk

                # æœ€å¤§100ãƒãƒ£ãƒ³ã‚¯ã¾ã§ãƒ†ã‚¹ãƒˆ
                if chunks_received >= 100:
                    break

            streaming_time = time.time() - start_time

            # çµæœã®æ¤œè¨¼
            assert chunks_received > 0, "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯ãŒå—ä¿¡ã•ã‚Œãªã‹ã£ãŸ"
            assert total_content.strip(), "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å†…å®¹ãŒç©º"

            test_result["tests"]["streaming_analysis"] = {
                "status": "passed",
                "streaming_time": streaming_time,
                "chunks_received": chunks_received,
                "total_content_length": len(total_content),
            }

            console.print(f"    âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æˆåŠŸ ({chunks_received}ãƒãƒ£ãƒ³ã‚¯, {streaming_time:.2f}ç§’)")

        except NotImplementedError:
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„å ´åˆ
            test_result["tests"]["streaming_analysis"] = {
                "status": "skipped",
                "reason": "ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“",
            }
            console.print("    â­ï¸  ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éå¯¾å¿œ")

        except Exception as e:
            test_result["tests"]["streaming_analysis"] = {"status": "failed", "error": str(e)}
            console.print(f"    âŒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¤±æ•—: {e}")
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®å¤±æ•—ã¯è‡´å‘½çš„ã§ã¯ãªã„ã®ã§ç¶™ç¶š

    async def _test_fix_suggestions(self, ai_integration: AIIntegration, provider: str, test_result: dict) -> None:
        """ä¿®æ­£ææ¡ˆãƒ†ã‚¹ãƒˆ"""
        console.print("  ğŸ”§ ä¿®æ­£ææ¡ˆãƒ†ã‚¹ãƒˆ...")

        start_time = time.time()

        try:
            options = AnalyzeOptions(
                provider=provider,
                generate_fixes=True,
                use_cache=False,
            )

            result = await ai_integration.analyze_log(self.test_log_content, options)

            fix_time = time.time() - start_time

            # ä¿®æ­£ææ¡ˆã®æ¤œè¨¼
            fix_count = len(result.fix_suggestions) if result.fix_suggestions else 0

            test_result["tests"]["fix_suggestions"] = {
                "status": "passed",
                "fix_time": fix_time,
                "fix_suggestions_count": fix_count,
            }

            console.print(f"    âœ… ä¿®æ­£ææ¡ˆæˆåŠŸ ({fix_count}å€‹ã®ææ¡ˆ, {fix_time:.2f}ç§’)")

        except Exception as e:
            test_result["tests"]["fix_suggestions"] = {"status": "failed", "error": str(e)}
            console.print(f"    âŒ ä¿®æ­£ææ¡ˆå¤±æ•—: {e}")
            # ä¿®æ­£ææ¡ˆã®å¤±æ•—ã¯è‡´å‘½çš„ã§ã¯ãªã„ã®ã§ç¶™ç¶š

    async def _test_interactive_mode(self, ai_integration: AIIntegration, provider: str, test_result: dict) -> None:
        """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ"""
        console.print("  ğŸ’¬ å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ...")

        start_time = time.time()

        try:
            options = AnalyzeOptions(
                provider=provider,
                streaming=False,
            )

            # å¯¾è©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹
            session = await ai_integration.start_interactive_session(self.test_log_content, options)

            # ãƒ†ã‚¹ãƒˆç”¨ã®è³ªå•ã‚’é€ä¿¡
            test_questions = [
                "ã“ã®ã‚¨ãƒ©ãƒ¼ã®åŸå› ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                "/help",  # ã‚³ãƒãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
            ]

            responses_received = 0

            for question in test_questions:
                try:
                    response_chunks = []
                    async for chunk in ai_integration.process_interactive_input(session.session_id, question):
                        response_chunks.append(chunk)

                    if response_chunks:
                        responses_received += 1

                except Exception as e:
                    console.print(f"    âš ï¸  è³ªå• '{question}' ã§ã‚¨ãƒ©ãƒ¼: {e}")

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†
            await ai_integration.close_interactive_session(session.session_id)

            interactive_time = time.time() - start_time

            test_result["tests"]["interactive_mode"] = {
                "status": "passed",
                "interactive_time": interactive_time,
                "questions_asked": len(test_questions),
                "responses_received": responses_received,
                "session_id": session.session_id,
            }

            console.print(
                f"    âœ… å¯¾è©±ãƒ¢ãƒ¼ãƒ‰æˆåŠŸ ({responses_received}/{len(test_questions)}å¿œç­”, {interactive_time:.2f}ç§’)"
            )

        except Exception as e:
            test_result["tests"]["interactive_mode"] = {"status": "failed", "error": str(e)}
            console.print(f"    âŒ å¯¾è©±ãƒ¢ãƒ¼ãƒ‰å¤±æ•—: {e}")
            # å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã®å¤±æ•—ã¯è‡´å‘½çš„ã§ã¯ãªã„ã®ã§ç¶™ç¶š

    async def _test_performance(self) -> None:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        console.print("\n[bold blue]2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ[/bold blue]")

        # åˆ©ç”¨å¯èƒ½ãªæœ€åˆã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
        available_provider = self._get_available_provider()
        if not available_provider:
            console.print("[yellow]âš ï¸  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“[/yellow]")
            return

        performance_results = {"provider": available_provider, "tests": {}, "overall_status": "unknown"}

        try:
            ai_integration = await self._create_ai_integration(available_provider)

            # å¤§ããªãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ
            await self._test_large_log_performance(ai_integration, available_provider, performance_results)

            # ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ
            await self._test_concurrent_analysis(ai_integration, available_provider, performance_results)

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ
            await self._test_memory_usage(ai_integration, available_provider, performance_results)

            performance_results["overall_status"] = "passed"
            console.print("[green]âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†[/green]")

        except Exception as e:
            performance_results["overall_status"] = "failed"
            performance_results["error_message"] = str(e)
            console.print(f"[red]âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}[/red]")
            if self.verbose:
                console.print_exception()

        self.results["performance_tests"] = performance_results
        self._update_test_counts(performance_results["overall_status"])

    async def _test_large_log_performance(self, ai_integration: AIIntegration, provider: str, results: dict) -> None:
        """å¤§ããªãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        console.print("  ğŸ“Š å¤§ããªãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ...")

        # 1MBã®ãƒ­ã‚°ã‚’ç”Ÿæˆ
        large_log = self._generate_large_log_content(1.0)

        start_time = time.time()
        memory_before = self._get_memory_usage()

        try:
            options = AnalyzeOptions(
                provider=provider,
                use_cache=False,
            )

            result = await ai_integration.analyze_log(large_log, options)

            processing_time = time.time() - start_time
            memory_after = self._get_memory_usage()
            memory_used = memory_after - memory_before

            results["tests"]["large_log_performance"] = {
                "status": "passed",
                "log_size_mb": len(large_log) / (1024 * 1024),
                "processing_time": processing_time,
                "memory_used_mb": memory_used / (1024 * 1024),
                "tokens_processed": result.tokens_used.total_tokens if result.tokens_used else 0,
            }

            console.print(f"    âœ… å¤§ããªãƒ­ã‚°å‡¦ç†æˆåŠŸ ({processing_time:.2f}ç§’, {memory_used / 1024 / 1024:.1f}MB)")

        except Exception as e:
            results["tests"]["large_log_performance"] = {"status": "failed", "error": str(e)}
            console.print(f"    âŒ å¤§ããªãƒ­ã‚°å‡¦ç†å¤±æ•—: {e}")
            raise

    async def _test_concurrent_analysis(self, ai_integration: AIIntegration, provider: str, results: dict) -> None:
        """ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        console.print("  ğŸ”„ ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ...")

        start_time = time.time()

        try:
            # 3ã¤ã®ä¸¦åˆ—åˆ†æã‚’å®Ÿè¡Œ
            options = AnalyzeOptions(
                provider=provider,
                use_cache=False,
            )

            tasks = []
            for i in range(3):
                # å„ã‚¿ã‚¹ã‚¯ã§å°‘ã—ç•°ãªã‚‹ãƒ­ã‚°ã‚’ä½¿ç”¨
                modified_log = self.test_log_content + f"\n# Test case {i}"
                task = ai_integration.analyze_log(modified_log, options)
                tasks.append(task)

            # ä¸¦åˆ—å®Ÿè¡Œ
            concurrent_results = await asyncio.gather(*tasks, return_exceptions=True)

            concurrent_time = time.time() - start_time

            # çµæœã®æ¤œè¨¼
            successful_analyses = sum(1 for r in concurrent_results if not isinstance(r, Exception))

            results["tests"]["concurrent_analysis"] = {
                "status": "passed",
                "concurrent_time": concurrent_time,
                "parallel_tasks": len(tasks),
                "successful_analyses": successful_analyses,
                "failed_analyses": len(tasks) - successful_analyses,
            }

            console.print(f"    âœ… ä¸¦åˆ—å‡¦ç†æˆåŠŸ ({successful_analyses}/{len(tasks)}æˆåŠŸ, {concurrent_time:.2f}ç§’)")

        except Exception as e:
            results["tests"]["concurrent_analysis"] = {"status": "failed", "error": str(e)}
            console.print(f"    âŒ ä¸¦åˆ—å‡¦ç†å¤±æ•—: {e}")
            # ä¸¦åˆ—å‡¦ç†ã®å¤±æ•—ã¯è‡´å‘½çš„ã§ã¯ãªã„ã®ã§ç¶™ç¶š

    async def _test_memory_usage(self, ai_integration: AIIntegration, provider: str, results: dict) -> None:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ"""
        console.print("  ğŸ§  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ...")

        try:
            memory_before = self._get_memory_usage()

            # è¤‡æ•°å›ã®åˆ†æã§ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
            for i in range(5):
                options = AnalyzeOptions(
                    provider=provider,
                    use_cache=False,
                )

                await ai_integration.analyze_log(self.test_log_content, options)

            memory_after = self._get_memory_usage()
            memory_increase = memory_after - memory_before

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒç•°å¸¸ã«å¢—åŠ ã—ã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            memory_increase_mb = memory_increase / (1024 * 1024)
            is_memory_leak = memory_increase_mb > 100  # 100MBä»¥ä¸Šã®å¢—åŠ ã¯ç•°å¸¸

            results["tests"]["memory_usage"] = {
                "status": "failed" if is_memory_leak else "passed",
                "memory_before_mb": memory_before / (1024 * 1024),
                "memory_after_mb": memory_after / (1024 * 1024),
                "memory_increase_mb": memory_increase_mb,
                "potential_memory_leak": is_memory_leak,
            }

            if is_memory_leak:
                console.print(f"    âš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ  ({memory_increase_mb:.1f}MB)")
            else:
                console.print(f"    âœ… ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ­£å¸¸ ({memory_increase_mb:.1f}MBå¢—åŠ )")

        except Exception as e:
            results["tests"]["memory_usage"] = {"status": "failed", "error": str(e)}
            console.print(f"    âŒ ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
            # ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆã®å¤±æ•—ã¯è‡´å‘½çš„ã§ã¯ãªã„ã®ã§ç¶™ç¶š

    async def _test_error_recovery(self) -> None:
        """ã‚¨ãƒ©ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
        console.print("\n[bold blue]3. ã‚¨ãƒ©ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆ[/bold blue]")

        recovery_results = {"tests": {}, "overall_status": "unknown"}

        try:
            # ç„¡åŠ¹ãªAPIã‚­ãƒ¼ãƒ†ã‚¹ãƒˆ
            await self._test_invalid_api_key_recovery(recovery_results)

            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ
            await self._test_network_error_recovery(recovery_results)

            # ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ãƒ†ã‚¹ãƒˆ
            await self._test_token_limit_recovery(recovery_results)

            recovery_results["overall_status"] = "passed"
            console.print("[green]âœ… ã‚¨ãƒ©ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆå®Œäº†[/green]")

        except Exception as e:
            recovery_results["overall_status"] = "failed"
            recovery_results["error_message"] = str(e)
            console.print(f"[red]âŒ ã‚¨ãƒ©ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}[/red]")
            if self.verbose:
                console.print_exception()

        self.results["error_recovery_tests"] = recovery_results
        self._update_test_counts(recovery_results["overall_status"])

    async def _test_invalid_api_key_recovery(self, results: dict) -> None:
        """ç„¡åŠ¹ãªAPIã‚­ãƒ¼ã‹ã‚‰ã®å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
        console.print("  ğŸ”‘ ç„¡åŠ¹ãªAPIã‚­ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆ...")

        try:
            # ç„¡åŠ¹ãªAPIã‚­ãƒ¼ã§AIçµ±åˆã‚’ä½œæˆ
            invalid_config = self._create_test_config("openai", api_key="invalid_key")
            ai_integration = AIIntegration(invalid_config)

            options = AnalyzeOptions(provider="openai")

            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            try:
                await ai_integration.analyze_log(self.test_log_content, options)
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã‹ã£ãŸå ´åˆã¯å¤±æ•—
                results["tests"]["invalid_api_key_recovery"] = {
                    "status": "failed",
                    "error": "ç„¡åŠ¹ãªAPIã‚­ãƒ¼ã§ã‚‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã›ã‚“ã§ã—ãŸ",
                }
                console.print("    âŒ ç„¡åŠ¹ãªAPIã‚­ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã›ã‚“ã§ã—ãŸ")
            except Exception as expected_error:
                # é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯æˆåŠŸ
                results["tests"]["invalid_api_key_recovery"] = {
                    "status": "passed",
                    "expected_error": str(expected_error),
                    "error_type": type(expected_error).__name__,
                }
                console.print(f"    âœ… é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {type(expected_error).__name__}")

        except Exception as e:
            results["tests"]["invalid_api_key_recovery"] = {"status": "failed", "error": str(e)}
            console.print(f"    âŒ ç„¡åŠ¹ãªAPIã‚­ãƒ¼ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    async def _test_network_error_recovery(self, results: dict) -> None:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
        console.print("  ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆ...")

        try:
            # ç„¡åŠ¹ãªãƒ™ãƒ¼ã‚¹URLã§AIçµ±åˆã‚’ä½œæˆ
            invalid_config = self._create_test_config("openai", base_url="https://invalid-url.example.com")
            ai_integration = AIIntegration(invalid_config)

            options = AnalyzeOptions(provider="openai")

            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            try:
                await ai_integration.analyze_log(self.test_log_content, options)
                results["tests"]["network_error_recovery"] = {
                    "status": "failed",
                    "error": "ç„¡åŠ¹ãªURLã§ã‚‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã›ã‚“ã§ã—ãŸ",
                }
                console.print("    âŒ ç„¡åŠ¹ãªURLã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã›ã‚“ã§ã—ãŸ")
            except Exception as expected_error:
                results["tests"]["network_error_recovery"] = {
                    "status": "passed",
                    "expected_error": str(expected_error),
                    "error_type": type(expected_error).__name__,
                }
                console.print(f"    âœ… é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {type(expected_error).__name__}")

        except Exception as e:
            results["tests"]["network_error_recovery"] = {"status": "failed", "error": str(e)}
            console.print(f"    âŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    async def _test_token_limit_recovery(self, results: dict) -> None:
        """ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
        console.print("  ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¾©æ—§ãƒ†ã‚¹ãƒˆ...")

        available_provider = self._get_available_provider()
        if not available_provider:
            results["tests"]["token_limit_recovery"] = {
                "status": "skipped",
                "reason": "åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“",
            }
            console.print("    â­ï¸  ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãªã—ã§ã‚¹ã‚­ãƒƒãƒ—")
            return

        try:
            # éå¸¸ã«å¤§ããªãƒ­ã‚°ã‚’ç”Ÿæˆï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚’è¶…ãˆã‚‹å¯èƒ½æ€§ï¼‰
            very_large_log = self._generate_large_log_content(5.0)  # 5MB

            ai_integration = await self._create_ai_integration(available_provider)
            options = AnalyzeOptions(provider=available_provider)

            try:
                result = await ai_integration.analyze_log(very_large_log, options)
                # æˆåŠŸã—ãŸå ´åˆï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å†…ã ã£ãŸï¼‰
                results["tests"]["token_limit_recovery"] = {
                    "status": "passed",
                    "result": "å¤§ããªãƒ­ã‚°ã‚‚æ­£å¸¸ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸ",
                    "tokens_used": result.tokens_used.total_tokens if result.tokens_used else 0,
                }
                console.print("    âœ… å¤§ããªãƒ­ã‚°ã‚‚æ­£å¸¸ã«å‡¦ç†")
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆï¼ˆæœŸå¾…ã•ã‚Œã‚‹å‹•ä½œï¼‰
                results["tests"]["token_limit_recovery"] = {
                    "status": "passed",
                    "expected_error": str(e),
                    "error_type": type(e).__name__,
                }
                console.print(f"    âœ… é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {type(e).__name__}")

        except Exception as e:
            results["tests"]["token_limit_recovery"] = {"status": "failed", "error": str(e)}
            console.print(f"    âŒ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    async def _create_ai_integration(self, provider: str) -> AIIntegration:
        """AIçµ±åˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ"""
        config = self._create_test_config(provider)
        ai_integration = AIIntegration(config)
        await ai_integration.initialize()
        return ai_integration

    def _create_test_config(self, provider: str, api_key: str | None = None, base_url: str | None = None) -> AIConfig:
        """ãƒ†ã‚¹ãƒˆç”¨ã®AIè¨­å®šã‚’ä½œæˆ"""
        providers = {}

        if provider == "openai":
            providers["openai"] = ProviderConfig(
                name="openai",
                api_key=api_key or os.getenv("OPENAI_API_KEY", ""),
                base_url=base_url,
                default_model="gpt-4o-mini",  # ãƒ†ã‚¹ãƒˆç”¨ã«å®‰ä¾¡ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                available_models=["gpt-4o-mini", "gpt-4o"],
                timeout_seconds=30,
                max_retries=3,
            )
        elif provider == "anthropic":
            providers["anthropic"] = ProviderConfig(
                name="anthropic",
                api_key=api_key or os.getenv("ANTHROPIC_API_KEY", ""),
                base_url=base_url,
                default_model="claude-3-5-haiku-20241022",  # ãƒ†ã‚¹ãƒˆç”¨ã«å®‰ä¾¡ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                available_models=["claude-3-5-haiku-20241022", "claude-3-5-sonnet-20241022"],
                timeout_seconds=30,
                max_retries=3,
            )
        elif provider == "local":
            providers["local"] = ProviderConfig(
                name="local",
                api_key="",
                base_url=base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                default_model="llama3.2",
                available_models=["llama3.2", "codellama"],
                timeout_seconds=60,
                max_retries=2,
            )

        return AIConfig(
            default_provider=provider,
            providers=providers,
            cache_enabled=True,
            cache_ttl_hours=1,  # ãƒ†ã‚¹ãƒˆç”¨ã«çŸ­ã„ TTL
            cache_max_size_mb=50,  # ãƒ†ã‚¹ãƒˆç”¨ã«å°ã•ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥
            cost_limits={"monthly_usd": 10.0, "per_request_usd": 1.0},  # ãƒ†ã‚¹ãƒˆç”¨ã«ä½ã„åˆ¶é™
            prompt_templates={},
            interactive_timeout=60,  # ãƒ†ã‚¹ãƒˆç”¨ã«çŸ­ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )

    def _get_available_provider(self) -> str | None:
        """åˆ©ç”¨å¯èƒ½ãªæœ€åˆã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—"""
        if os.getenv("OPENAI_API_KEY"):
            return "openai"
        elif os.getenv("ANTHROPIC_API_KEY"):
            return "anthropic"
        elif self._check_local_ollama():
            return "local"
        return None

    def _check_local_ollama(self) -> bool:
        """ãƒ­ãƒ¼ã‚«ãƒ«OllamaãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            # ç°¡å˜ãªæ¥ç¶šãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯éåŒæœŸã§è¡Œã†ï¼‰
            return True  # ç°¡ç•¥åŒ–
        except Exception:
            return False

    def _get_memory_usage(self) -> int:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆãƒã‚¤ãƒˆï¼‰"""
        try:
            import psutil

            process = psutil.Process()
            return process.memory_info().rss
        except ImportError:
            # psutilãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯0ã‚’è¿”ã™
            return 0

    def _update_test_counts(self, status: str) -> None:
        """ãƒ†ã‚¹ãƒˆçµæœã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°"""
        self.results["summary"]["total_tests"] += 1
        if status == "passed":
            self.results["summary"]["passed_tests"] += 1
        elif status == "failed":
            self.results["summary"]["failed_tests"] += 1

    def _generate_summary_report(self) -> None:
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        console.print("\n" + "=" * 60)
        console.print(Panel.fit("ğŸ“Š å®Ÿç’°å¢ƒãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼", style="bold blue"))

        summary = self.results["summary"]
        total_time = summary["end_time"] - summary["start_time"] if summary["end_time"] and summary["start_time"] else 0

        # å…¨ä½“çµ±è¨ˆ
        stats_table = Table(title="ğŸ“ˆ å…¨ä½“çµ±è¨ˆ", show_header=True, header_style="bold green")
        stats_table.add_column("é …ç›®", style="cyan")
        stats_table.add_column("å€¤", style="white")

        stats_table.add_row("ç·ãƒ†ã‚¹ãƒˆæ•°", str(summary["total_tests"]))
        stats_table.add_row("æˆåŠŸ", f"[green]{summary['passed_tests']}[/green]")
        stats_table.add_row("å¤±æ•—", f"[red]{summary['failed_tests']}[/red]")
        stats_table.add_row("æˆåŠŸç‡", f"{(summary['passed_tests'] / max(summary['total_tests'], 1) * 100):.1f}%")
        stats_table.add_row("å®Ÿè¡Œæ™‚é–“", f"{total_time:.2f}ç§’")

        console.print(stats_table)

        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥çµæœ
        if self.results["provider_tests"]:
            provider_table = Table(title="ğŸ¤– ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥çµæœ", show_header=True, header_style="bold blue")
            provider_table.add_column("ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼", style="cyan")
            provider_table.add_column("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", style="white")
            provider_table.add_column("ãƒ†ã‚¹ãƒˆæ•°", style="white")
            provider_table.add_column("å‚™è€ƒ", style="dim")

            for provider, result in self.results["provider_tests"].items():
                status_color = "green" if result["overall_status"] == "passed" else "red"
                test_count = len(result["tests"])
                note = result.get("error_message", "æ­£å¸¸")[:50]

                provider_table.add_row(
                    provider.upper(),
                    f"[{status_color}]{result['overall_status']}[/{status_color}]",
                    str(test_count),
                    note,
                )

            console.print(provider_table)

        # æ¨å¥¨äº‹é …
        console.print(Panel.fit("ğŸ’¡ æ¨å¥¨äº‹é …", style="yellow"))

        recommendations = []

        if summary["failed_tests"] > 0:
            recommendations.append("å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®è©³ç´°ã‚’ç¢ºèªã—ã€è¨­å®šã‚„APIã‚­ãƒ¼ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„")

        if not self.results["provider_tests"]:
            recommendations.append("APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

        if len(self.results["provider_tests"]) < 2:
            recommendations.append("è¤‡æ•°ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¨­å®šã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„")

        if not recommendations:
            recommendations.append("å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼æœ¬ç•ªç’°å¢ƒã§ã®ä½¿ç”¨æº–å‚™ãŒæ•´ã£ã¦ã„ã¾ã™ã€‚")

        for i, rec in enumerate(recommendations, 1):
            console.print(f"  {i}. {rec}")

        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        self._save_results_to_file()

    def _save_results_to_file(self) -> None:
        """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            results_dir = Path("test_results")
            results_dir.mkdir(exist_ok=True)

            timestamp = time.strftime("%Y%m%d_%H%M%S")
            results_file = results_dir / f"real_environment_test_{timestamp}.json"

            with results_file.open("w", encoding="utf-8") as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)

            console.print(f"\n[dim]ğŸ“„ è©³ç´°çµæœã‚’ä¿å­˜: {results_file}[/dim]")

        except Exception as e:
            console.print(f"[yellow]âš ï¸  çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—: {e}[/yellow]")


@click.command()
@click.option(
    "--provider",
    type=click.Choice(["openai", "anthropic", "local"], case_sensitive=False),
    help="ãƒ†ã‚¹ãƒˆã™ã‚‹ç‰¹å®šã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯å…¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼‰",
)
@click.option("--verbose", "-v", is_flag=True, help="è©³ç´°ãªãƒ­ã‚°ã‚’è¡¨ç¤º")
def main(provider: str | None, verbose: bool) -> None:
    """å®Ÿç’°å¢ƒã§ã®å‹•ä½œç¢ºèªã‚’å®Ÿè¡Œ

    AIçµ±åˆæ©Ÿèƒ½ã®å®Ÿéš›ã®APIã‚­ãƒ¼ã‚’ä½¿ç”¨ã—ãŸE2Eå‹•ä½œç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚

    \b
    å¿…è¦ãªç’°å¢ƒå¤‰æ•°:
      OPENAI_API_KEY     - OpenAI APIã‚­ãƒ¼
      ANTHROPIC_API_KEY  - Anthropic APIã‚­ãƒ¼
      OLLAMA_BASE_URL    - ãƒ­ãƒ¼ã‚«ãƒ«LLMã®URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    \b
    ä½¿ç”¨ä¾‹:
      uv run python scripts/test_real_environment.py
      uv run python scripts/test_real_environment.py --provider openai
      uv run python scripts/test_real_environment.py --verbose
    """

    async def run_tests():
        tester = RealEnvironmentTester(verbose=verbose)

        try:
            results = await tester.run_all_tests(provider)

            # çµ‚äº†ã‚³ãƒ¼ãƒ‰ã‚’æ±ºå®š
            if results["summary"]["failed_tests"] > 0:
                console.print("\n[red]âŒ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ[/red]")
                sys.exit(1)
            else:
                console.print("\n[green]âœ… å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ[/green]")
                sys.exit(0)

        except KeyboardInterrupt:
            console.print("\n[yellow]âš ï¸  ãƒ†ã‚¹ãƒˆãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ[/yellow]")
            sys.exit(130)
        except Exception as e:
            console.print(f"\n[red]âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}[/red]")
            if verbose:
                console.print_exception()
            sys.exit(1)

    # ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
    available_keys = []
    if os.getenv("OPENAI_API_KEY"):
        available_keys.append("OPENAI_API_KEY")
    if os.getenv("ANTHROPIC_API_KEY"):
        available_keys.append("ANTHROPIC_API_KEY")
    if os.getenv("OLLAMA_BASE_URL"):
        available_keys.append("OLLAMA_BASE_URL")

    if not available_keys and not provider == "local":
        console.print("[red]âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“[/red]")
        console.print("\n[blue]ğŸ’¡ ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„:[/blue]")
        console.print("  export OPENAI_API_KEY=your_openai_key")
        console.print("  export ANTHROPIC_API_KEY=your_anthropic_key")
        console.print("  export OLLAMA_BASE_URL=http://localhost:11434  # ãƒ­ãƒ¼ã‚«ãƒ«LLMç”¨")
        console.print("\n[dim]ã¾ãŸã¯ .env ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰ã—ã¦ãã ã•ã„[/dim]")
        sys.exit(1)

    console.print(f"[green]âœ… åˆ©ç”¨å¯èƒ½ãªAPIã‚­ãƒ¼: {', '.join(available_keys)}[/green]")

    # éåŒæœŸå®Ÿè¡Œ
    asyncio.run(run_tests())


if __name__ == "__main__":
    main()
