"""
ãƒ­ã‚°æ•´å½¢æ©Ÿèƒ½çµ±åˆãƒ†ã‚¹ãƒˆ

ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠæ–¹å¼ã¨ã‚³ãƒãƒ³ãƒ‰æŒ‡å®šå®Ÿè¡Œæ–¹å¼ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè£…ã—ã¾ã™ã€‚
è¦ä»¶11.1-11.5ã«å¯¾å¿œã—ãŸåŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚
"""

from __future__ import annotations

import json

# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch

import pytest
from ci_helper.cli import cli
from ci_helper.core.models import ExecutionResult, Failure, FailureType, JobResult, WorkflowResult
from ci_helper.formatters import get_formatter_manager
from ci_helper.ui.command_menus import CommandMenuBuilder
from ci_helper.ui.menu_system import MenuSystem
from click.testing import CliRunner
from rich.console import Console

sys.path.insert(0, str(Path(__file__).parent.parent))
from tests.utils.mock_helpers import setup_stable_prompt_mock


class TestLogFormattingIntegration:
    """ãƒ­ã‚°æ•´å½¢æ©Ÿèƒ½ã®çµ±åˆãƒ†ã‚¹ãƒˆ

    è¦ä»¶11.1-11.5ã«å¯¾å¿œ:
    - 11.1: ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠæ–¹å¼ã¨ã‚³ãƒãƒ³ãƒ‰æŒ‡å®šå®Ÿè¡Œæ–¹å¼ã®çµ±åˆãƒ†ã‚¹ãƒˆ
    - 11.2: åŒã˜æ•´å½¢ã‚¨ãƒ³ã‚¸ãƒ³ã®ä½¿ç”¨ç¢ºèªãƒ†ã‚¹ãƒˆ
    - 11.3: åŒã˜å‡ºåŠ›å“è³ªã®ç¢ºèªãƒ†ã‚¹ãƒˆ
    - 11.4: ãƒãƒƒãƒå‡¦ç†ã¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ±åˆã®ãƒ†ã‚¹ãƒˆ
    - 11.5: å¯¾è©±çš„æ¢ç´¢æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
    """

    @pytest.fixture
    def sample_execution_result(self) -> ExecutionResult:
        """ãƒ†ã‚¹ãƒˆç”¨ã®å®Ÿè¡Œçµæœã‚’ä½œæˆ"""
        failures = [
            Failure(
                type=FailureType.ASSERTION,
                message="AssertionError: Expected 'production' but got 'development'",
                file_path="tests/unit/test_config.py",
                line_number=45,
                context_before=["def test_load_config():", "    config = load_config('config.toml')"],
                context_after=["    assert config.debug == False"],
                stack_trace='Traceback (most recent call last):\n  File "test_config.py", line 45',
            ),
            Failure(
                type=FailureType.ERROR,
                message="ModuleNotFoundError: No module named 'missing_module'",
                file_path="src/app/main.py",
                line_number=12,
                context_before=["import os", "import sys"],
                context_after=["from app.config import Config"],
                stack_trace=None,
            ),
        ]

        job_result = JobResult(
            name="test",
            success=False,
            failures=failures,
            duration=45.2,
        )

        workflow_result = WorkflowResult(
            name="test.yml",
            success=False,
            jobs=[job_result],
            duration=45.2,
        )

        return ExecutionResult(
            success=False,
            workflows=[workflow_result],
            total_duration=45.2,
        )

    @pytest.fixture
    def temp_log_file(self, sample_execution_result: ExecutionResult) -> Path:
        """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".log", delete=False) as f:
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ­ã‚°å†…å®¹ã‚’æ›¸ãè¾¼ã¿
            f.write("CIå®Ÿè¡Œãƒ­ã‚°\n")
            f.write("Job 'test' failed\n")
            f.write("AssertionError: Expected 'production' but got 'development'\n")
            f.write("ModuleNotFoundError: No module named 'missing_module'\n")
            temp_path = Path(f.name)

        yield temp_path

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if temp_path.exists():
            temp_path.unlink()

    def test_menu_and_command_use_same_formatter_engine(self, sample_execution_result: ExecutionResult):
        """è¦ä»¶11.2: ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã¨ã‚³ãƒãƒ³ãƒ‰ãŒåŒã˜æ•´å½¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        formatter_manager = get_formatter_manager()

        # å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå½¢å¼ã§åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        ai_formatter_1 = formatter_manager.get_formatter("ai")
        ai_formatter_2 = formatter_manager.get_formatter("ai")

        human_formatter_1 = formatter_manager.get_formatter("human")
        human_formatter_2 = formatter_manager.get_formatter("human")

        json_formatter_1 = formatter_manager.get_formatter("json")
        json_formatter_2 = formatter_manager.get_formatter("json")

        # åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert type(ai_formatter_1) is type(ai_formatter_2)
        assert type(human_formatter_1) is type(human_formatter_2)
        assert type(json_formatter_1) is type(json_formatter_2)

        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ãŒæ­£ã—ãç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        available_formats = formatter_manager.list_available_formats()
        assert "ai" in available_formats
        assert "human" in available_formats
        assert "json" in available_formats

    def test_same_output_quality_between_menu_and_command(
        self, sample_execution_result: ExecutionResult, temp_log_file: Path
    ):
        """è¦ä»¶11.3: ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã¨ã‚³ãƒãƒ³ãƒ‰ã§åŒã˜å‡ºåŠ›å“è³ªã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        formatter_manager = get_formatter_manager()

        # å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå½¢å¼ã§å‡ºåŠ›å†…å®¹ã‚’æ¯”è¼ƒ
        formats_to_test = ["ai", "human", "json"]

        for format_type in formats_to_test:
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰ç›´æ¥å–å¾—ï¼ˆãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ»ã‚³ãƒãƒ³ãƒ‰å…±é€šï¼‰
            formatted_output = formatter_manager.format_log(
                sample_execution_result, format_type, filter_errors=False, verbose_level="normal"
            )

            # å‡ºåŠ›ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
            assert formatted_output.strip(), f"{format_type}ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å‡ºåŠ›ãŒç©ºã§ã™"

            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå›ºæœ‰ã®å†…å®¹ãƒã‚§ãƒƒã‚¯
            if format_type == "ai":
                # AIå½¢å¼ã§ã¯æ§‹é€ åŒ–ã•ã‚ŒãŸMarkdownãŒç”Ÿæˆã•ã‚Œã‚‹
                assert "# CI Failure Report" in formatted_output or "## " in formatted_output
                assert "AssertionError" in formatted_output
                assert "test_config.py" in formatted_output

            elif format_type == "human":
                # äººé–“å¯èª­å½¢å¼ã§ã¯ Rich ã®ãƒœãƒƒã‚¯ã‚¹æ–‡å­—ã‚„Unicodeæ–‡å­—ãŒå«ã¾ã‚Œã‚‹
                # Rich markup ã®ä»£ã‚ã‚Šã«ã€å®Ÿéš›ã®å‡ºåŠ›ã«å«ã¾ã‚Œã‚‹æ–‡å­—ã‚’ãƒã‚§ãƒƒã‚¯
                assert (
                    "â•­" in formatted_output
                    or "â”‚" in formatted_output
                    or "ğŸ¯" in formatted_output
                    or "âŒ" in formatted_output
                )  # Rich box drawing or emoji

            elif format_type == "json":
                # JSONå½¢å¼ã§ã¯æœ‰åŠ¹ãªJSONãŒç”Ÿæˆã•ã‚Œã‚‹
                try:
                    parsed_json = json.loads(formatted_output)
                    assert isinstance(parsed_json, dict)
                    # Check for the actual JSON structure used by the formatter
                    assert "success" in parsed_json or (
                        "execution_summary" in parsed_json and "success" in parsed_json["execution_summary"]
                    )
                    assert "workflows" in parsed_json or "all_failures" in parsed_json
                except json.JSONDecodeError:
                    pytest.fail(f"JSONå½¢å¼ã®å‡ºåŠ›ãŒç„¡åŠ¹ã§ã™: {formatted_output[:200]}...")

    @patch("ci_helper.commands.format_logs._get_execution_result")
    def test_command_line_execution_integration(
        self, mock_get_result: Mock, sample_execution_result: ExecutionResult, temp_log_file: Path
    ):
        """è¦ä»¶11.1, 11.4: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œæ–¹å¼ã®çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆãƒãƒƒãƒå‡¦ç†å¯¾å¿œï¼‰"""
        mock_get_result.return_value = sample_execution_result

        runner = CliRunner()

        # AIå½¢å¼ã§ã®ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        with patch("ci_helper.commands.format_logs.get_formatter_manager") as mock_manager:
            mock_formatter_manager = Mock()
            mock_formatter_manager.format_log.return_value = "# AI Formatted Output\n\nTest content"
            mock_manager.return_value = mock_formatter_manager

            with patch("ci_helper.commands.format_logs.get_progress_manager") as mock_progress:
                mock_progress_manager = Mock()
                mock_progress_manager.execute_with_progress.return_value = "# AI Formatted Output\n\nTest content"
                mock_progress.return_value = mock_progress_manager

                result = runner.invoke(cli, ["format-logs", "--format", "ai", "--input", str(temp_log_file)])

                # ã‚³ãƒãƒ³ãƒ‰ãŒæ­£å¸¸å®Ÿè¡Œã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
                assert result.exit_code == 0
                assert "# AI Formatted Output" in result.output

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒæ­£ã—ãå‘¼ã³å‡ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆformat_logã¯å†…éƒ¨ã§å‘¼ã°ã‚Œã‚‹ï¼‰
                mock_progress_manager.execute_with_progress.assert_called_once()

    @patch("ci_helper.commands.format_logs._get_execution_result")
    def test_batch_processing_with_multiple_formats(
        self, mock_get_result: Mock, sample_execution_result: ExecutionResult, temp_log_file: Path
    ):
        """è¦ä»¶11.4: ãƒãƒƒãƒå‡¦ç†ã§ã®è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›ãƒ†ã‚¹ãƒˆ"""
        mock_get_result.return_value = sample_execution_result

        runner = CliRunner()
        formats = ["ai", "human", "json"]

        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)

            # å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚’ãƒ†ã‚¹ãƒˆ
            for format_type in formats:
                output_file = temp_path / f"output.{format_type}"

                with patch("ci_helper.commands.format_logs.get_formatter_manager") as mock_manager:
                    mock_formatter_manager = Mock()
                    mock_formatter_manager.format_log.return_value = f"# {format_type.upper()} Output\n\nContent"
                    mock_manager.return_value = mock_formatter_manager

                    with patch("ci_helper.commands.format_logs.get_progress_manager") as mock_progress:
                        mock_progress_manager = Mock()
                        mock_progress_manager.execute_with_progress.return_value = (
                            f"# {format_type.upper()} Output\n\nContent"
                        )
                        mock_progress.return_value = mock_progress_manager

                        with patch("ci_helper.utils.file_save_utils.FileSaveManager.save_formatted_log") as mock_save:
                            mock_save.return_value = (True, str(output_file))

                            result = runner.invoke(
                                cli,
                                [
                                    "format-logs",
                                    "--format",
                                    format_type,
                                    "--input",
                                    str(temp_log_file),
                                    "--output",
                                    str(output_file),
                                    "--no-confirm",
                                ],
                            )

                            # ãƒãƒƒãƒå‡¦ç†ãŒæˆåŠŸã™ã‚‹ã“ã¨ã‚’ç¢ºèª
                            assert result.exit_code == 0
                            mock_save.assert_called_once()

    @patch("rich.prompt.Prompt.ask")
    def test_menu_interactive_exploration(self, mock_prompt: Mock, sample_execution_result: ExecutionResult):
        """è¦ä»¶11.5: å¯¾è©±çš„æ¢ç´¢æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        console = Console()

        # ãƒ¢ãƒƒã‚¯ã‚³ãƒãƒ³ãƒ‰ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ä½œæˆ
        command_handlers = {
            "format_logs": Mock(return_value=True),
            "format_logs_custom": Mock(return_value=True),
        }

        builder = CommandMenuBuilder(console, command_handlers)
        menu_system = MenuSystem(console)

        # ãƒ­ã‚°æ•´å½¢ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã¸ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        # ãƒ­ã‚°ç®¡ç† â†’ ãƒ­ã‚°æ•´å½¢ â†’ AIåˆ†æç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ â†’ æˆ»ã‚‹ â†’ æˆ»ã‚‹ â†’ æˆ»ã‚‹ â†’ çµ‚äº†
        # å®‰å®šã—ãŸãƒ¢ãƒƒã‚¯è¨­å®šã‚’ä½¿ç”¨ã—ã¦StopIterationã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
        setup_stable_prompt_mock(mock_prompt, ["5", "4", "1", "1", "", "b", "b", "b", "q"])

        main_menu = builder.build_main_menu()

        with patch.object(menu_system, "console") as mock_console:
            mock_console.clear = Mock()
            mock_console.print = Mock()
            menu_system.run_menu(main_menu)

        # format_logsãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå‘¼ã³å‡ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        command_handlers["format_logs"].assert_called_once()

    @patch("rich.prompt.Prompt.ask")
    @patch("rich.prompt.Confirm.ask")
    def test_menu_custom_formatting_exploration(self, mock_confirm: Mock, mock_prompt: Mock):
        """è¦ä»¶11.5: ã‚«ã‚¹ã‚¿ãƒ æ•´å½¢æ©Ÿèƒ½ã®å¯¾è©±çš„æ¢ç´¢ãƒ†ã‚¹ãƒˆ"""
        console = Console()

        # ãƒ¢ãƒƒã‚¯ã‚³ãƒãƒ³ãƒ‰ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ä½œæˆ
        command_handlers = {
            "format_logs_custom": Mock(return_value=True),
        }

        builder = CommandMenuBuilder(console, command_handlers)
        menu_system = MenuSystem(console)

        # ã‚«ã‚¹ã‚¿ãƒ æ•´å½¢ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã¸ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
        # ãƒ­ã‚°ç®¡ç† â†’ ãƒ­ã‚°æ•´å½¢ â†’ ã‚«ã‚¹ã‚¿ãƒ æ•´å½¢ â†’ è¨­å®š â†’ å®Ÿè¡Œç¢ºèª â†’ æˆ»ã‚‹
        # å®‰å®šã—ãŸãƒ¢ãƒƒã‚¯è¨­å®šã‚’ä½¿ç”¨ã—ã¦StopIterationã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
        setup_stable_prompt_mock(
            mock_prompt,
            [
                "5",  # ãƒ­ã‚°ç®¡ç†
                "4",  # ãƒ­ã‚°æ•´å½¢
                "3",  # ã‚«ã‚¹ã‚¿ãƒ æ•´å½¢
                "ai",  # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé¸æŠ
                "detailed",  # è©³ç´°ãƒ¬ãƒ™ãƒ«
                "y",  # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æœ‰åŠ¹
                "console",  # å‡ºåŠ›å…ˆé¸æŠ
                "b",  # æˆ»ã‚‹
                "b",  # æˆ»ã‚‹
                "b",  # æˆ»ã‚‹
                "q",  # çµ‚äº†
            ],
        )

        # Confirm.askã®ãƒ¢ãƒƒã‚¯è¨­å®šï¼ˆå®Ÿè¡Œç¢ºèªï¼‰
        mock_confirm.return_value = True

        main_menu = builder.build_main_menu()

        with patch.object(menu_system, "console") as mock_console:
            mock_console.clear = Mock()
            mock_console.print = Mock()

            # ã‚«ã‚¹ã‚¿ãƒ æ•´å½¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šç”»é¢ã®ãƒ¢ãƒƒã‚¯
            with patch.object(builder, "_show_custom_format_parameter_screen") as mock_param_screen:
                mock_param_screen.return_value = {
                    "format_type": "ai",
                    "detail_level": "detailed",
                    "filter_errors": True,
                    "advanced_options": {},
                }

                # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã®ãƒ¢ãƒƒã‚¯
                with patch.object(builder, "_select_log_file") as mock_select_log:
                    mock_select_log.return_value = "/mock/path/latest.log"

                    # è¨­å®šç¢ºèªç”»é¢ã®ãƒ¢ãƒƒã‚¯
                    with patch.object(builder, "_show_custom_format_confirmation") as mock_confirmation:
                        mock_confirmation.return_value = None

                        menu_system.run_menu(main_menu)

        # ã‚«ã‚¹ã‚¿ãƒ æ•´å½¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå‘¼ã³å‡ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        command_handlers["format_logs_custom"].assert_called_once()

    def test_formatter_consistency_across_execution_methods(self, sample_execution_result: ExecutionResult):
        """è¦ä»¶11.2, 11.3: å®Ÿè¡Œæ–¹å¼é–“ã§ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        formatter_manager = get_formatter_manager()

        # åŒã˜å…¥åŠ›ã«å¯¾ã—ã¦è¤‡æ•°å›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å®Ÿè¡Œ
        format_options = {"filter_errors": False, "verbose_level": "normal"}

        # AIå½¢å¼ã§ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ
        ai_output_1 = formatter_manager.format_log(sample_execution_result, "ai", **format_options)
        ai_output_2 = formatter_manager.format_log(sample_execution_result, "ai", **format_options)

        # åŒã˜å…¥åŠ›ã«å¯¾ã—ã¦åŒã˜å‡ºåŠ›ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert ai_output_1 == ai_output_2

        # JSONå½¢å¼ã§ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ
        json_output_1 = formatter_manager.format_log(sample_execution_result, "json", **format_options)
        json_output_2 = formatter_manager.format_log(sample_execution_result, "json", **format_options)

        # JSONå‡ºåŠ›ã®æ§‹é€ çš„ä¸€è²«æ€§ã‚’ç¢ºèªï¼ˆå‹•çš„ãªå€¤ã‚’é™¤å¤–ï¼‰
        import json

        data1 = json.loads(json_output_1)
        data2 = json.loads(json_output_2)

        # å‹•çš„ãªå€¤ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç­‰ï¼‰ã‚’é™¤å¤–ã—ã¦æ¯”è¼ƒ
        def remove_dynamic_values(data):
            """å‹•çš„ãªå€¤ã‚’é™¤å¤–ã—ãŸã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ"""
            import copy

            cleaned_data = copy.deepcopy(data)

            # format_infoå†…ã®generated_atã‚’é™¤å¤–
            if "format_info" in cleaned_data and "generated_at" in cleaned_data["format_info"]:
                del cleaned_data["format_info"]["generated_at"]

            return cleaned_data

        cleaned_data1 = remove_dynamic_values(data1)
        cleaned_data2 = remove_dynamic_values(data2)

        assert cleaned_data1 == cleaned_data2

        # ç•°ãªã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé–“ã§ã¯ç•°ãªã‚‹å‡ºåŠ›ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert ai_output_1 != json_output_1

    @patch("ci_helper.commands.format_logs._get_execution_result")
    def test_script_integration_with_exit_codes(
        self, mock_get_result: Mock, sample_execution_result: ExecutionResult, temp_log_file: Path
    ):
        """è¦ä»¶11.4: ã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ±åˆã§ã®çµ‚äº†ã‚³ãƒ¼ãƒ‰ç¢ºèªãƒ†ã‚¹ãƒˆ"""
        runner = CliRunner()

        # æˆåŠŸã‚±ãƒ¼ã‚¹
        mock_get_result.return_value = sample_execution_result

        with patch("ci_helper.commands.format_logs.get_formatter_manager") as mock_manager:
            mock_formatter_manager = Mock()
            mock_formatter_manager.format_log.return_value = "Formatted output"
            mock_manager.return_value = mock_formatter_manager

            with patch("ci_helper.commands.format_logs.get_progress_manager") as mock_progress:
                mock_progress_manager = Mock()
                mock_progress_manager.execute_with_progress.return_value = "Formatted output"
                mock_progress.return_value = mock_progress_manager

                result = runner.invoke(cli, ["format-logs", "--format", "ai", "--input", str(temp_log_file)])

                # æˆåŠŸæ™‚ã¯çµ‚äº†ã‚³ãƒ¼ãƒ‰0
                assert result.exit_code == 0

        # å¤±æ•—ã‚±ãƒ¼ã‚¹ï¼ˆãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ï¼‰
        mock_get_result.return_value = None

        with patch("ci_helper.commands.format_logs.get_progress_manager") as mock_progress:
            mock_progress_manager = Mock()
            mock_progress.return_value = mock_progress_manager

            result = runner.invoke(cli, ["format-logs", "--format", "ai", "--input", str(temp_log_file)])

            # å¤±æ•—æ™‚ã¯çµ‚äº†ã‚³ãƒ¼ãƒ‰1
            assert result.exit_code == 1

    def test_menu_and_command_parameter_compatibility(self):
        """è¦ä»¶11.1: ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã¨ã‚³ãƒãƒ³ãƒ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äº’æ›æ€§ãƒ†ã‚¹ãƒˆ"""
        formatter_manager = get_formatter_manager()

        # ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        menu_params = {"filter_errors": True, "verbose_level": "detailed"}

        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        command_params = {"filter_errors": True, "verbose_level": "detailed"}

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ§‹é€ ãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert menu_params == command_params

        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ãŒä¸¡æ–¹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å—ã‘å…¥ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        sample_result = ExecutionResult(success=True, workflows=[], total_duration=0.0)

        try:
            formatter_manager.format_log(sample_result, "ai", **menu_params)
            formatter_manager.format_log(sample_result, "ai", **command_params)
        except Exception as e:
            pytest.fail(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äº’æ›æ€§ã‚¨ãƒ©ãƒ¼: {e}")

    @patch("rich.prompt.Prompt.ask")
    def test_menu_error_handling_consistency(self, mock_prompt: Mock):
        """è¦ä»¶11.1: ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã¨ã‚³ãƒãƒ³ãƒ‰ã§ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        console = Console()

        # ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ãƒ¢ãƒƒã‚¯ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        error_handler = Mock(side_effect=Exception("ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼"))
        command_handlers = {
            "format_logs": error_handler,
        }

        builder = CommandMenuBuilder(console, command_handlers)
        menu_system = MenuSystem(console)

        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ãƒ¡ãƒ‹ãƒ¥ãƒ¼æ“ä½œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        # å®‰å®šã—ãŸãƒ¢ãƒƒã‚¯è¨­å®šã‚’ä½¿ç”¨ã—ã¦StopIterationã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
        setup_stable_prompt_mock(mock_prompt, ["5", "4", "1", "1", "", "q"])

        main_menu = builder.build_main_menu()

        with patch.object(menu_system, "console") as mock_console:
            mock_console.clear = Mock()
            mock_console.print = Mock()

            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãªã„ã“ã¨ã‚’ç¢ºèª
            try:
                menu_system.run_menu(main_menu)
            except Exception:
                pytest.fail("ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã§ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒé©åˆ‡ã«å‹•ä½œã—ã¦ã„ã¾ã›ã‚“")

        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå‘¼ã³å‡ºã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
        error_handler.assert_called_once()

    def test_output_format_validation_consistency(self):
        """è¦ä»¶11.2, 11.3: å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œè¨¼ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        formatter_manager = get_formatter_manager()

        # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        supported_formats = ["ai", "human", "json"]

        # å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒæ­£ã—ãç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        available_formats = formatter_manager.list_available_formats()
        for format_type in supported_formats:
            assert format_type in available_formats

        # ç„¡åŠ¹ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        sample_result = ExecutionResult(success=True, workflows=[], total_duration=0.0)

        # LogFormattingErrorãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆValueErrorã§ã¯ãªãï¼‰
        from ci_helper.core.exceptions import LogFormattingError

        with pytest.raises(LogFormattingError):
            formatter_manager.format_log(sample_result, "invalid_format")
