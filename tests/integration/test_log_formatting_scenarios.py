"""ãƒ­ã‚°æ•´å½¢æ©Ÿèƒ½ã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ

å®Ÿéš›ã®ä½¿ç”¨ã‚·ãƒŠãƒªã‚ªã«åŸºã¥ã„ãŸçµ±åˆãƒ†ã‚¹ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚
"""

from __future__ import annotations

import json

# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch

import pytest
from ci_helper.cli import cli
from ci_helper.core.models import ExecutionResult, Failure, FailureType, JobResult, WorkflowResult
from ci_helper.formatters import get_formatter_manager
from click.testing import CliRunner
from rich.console import Console

sys.path.insert(0, str(Path(__file__).parent.parent))
from tests.utils.mock_helpers import setup_stable_prompt_mock


class TestLogFormattingScenarios:
    """å®Ÿéš›ã®ä½¿ç”¨ã‚·ãƒŠãƒªã‚ªã«åŸºã¥ã„ãŸãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def complex_execution_result(self) -> ExecutionResult:
        """è¤‡é›‘ãªå®Ÿè¡Œçµæœã‚’ä½œæˆï¼ˆè¤‡æ•°ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€ã‚¸ãƒ§ãƒ–ã€å¤±æ•—ã‚’å«ã‚€ï¼‰"""
        failures_job1 = [
            Failure(
                type=FailureType.ASSERTION,
                message="AssertionError: Expected 200 but got 404",
                file_path="tests/integration/test_api.py",
                line_number=25,
                context_before=["def test_api_endpoint():", "    response = client.get('/api/users')"],
                context_after=["    assert response.json()['users']"],
                stack_trace='Traceback (most recent call last):\n  File "test_api.py", line 25',
            ),
            Failure(
                type=FailureType.ERROR,
                message="ConnectionError: Failed to connect to database",
                file_path="src/database/connection.py",
                line_number=15,
                context_before=["def connect():", "    try:"],
                context_after=["    except Exception as e:", "        raise ConnectionError(str(e))"],
                stack_trace='Traceback (most recent call last):\n  File "connection.py", line 15',
            ),
        ]

        failures_job2 = [
            Failure(
                type=FailureType.SYNTAX,
                message="SyntaxError: invalid syntax",
                file_path="src/utils/parser.py",
                line_number=42,
                context_before=["def parse_config(data):", "    if data:"],
                context_after=["        return parsed_data"],
                stack_trace=None,
            ),
        ]

        job1 = JobResult(name="test", success=False, failures=failures_job1, duration=120.5)
        job2 = JobResult(name="lint", success=False, failures=failures_job2, duration=30.2)
        job3 = JobResult(name="build", success=True, failures=[], duration=180.0)

        workflow1 = WorkflowResult(name="ci.yml", success=False, jobs=[job1, job2], duration=150.7)
        workflow2 = WorkflowResult(name="deploy.yml", success=True, jobs=[job3], duration=180.0)

        return ExecutionResult(success=False, workflows=[workflow1, workflow2], total_duration=330.7)

    @pytest.fixture
    def empty_execution_result(self) -> ExecutionResult:
        """ç©ºã®å®Ÿè¡Œçµæœï¼ˆå¤±æ•—ãªã—ï¼‰"""
        job = JobResult(name="test", success=True, failures=[], duration=45.0)
        workflow = WorkflowResult(name="test.yml", success=True, jobs=[job], duration=45.0)

        return ExecutionResult(success=True, workflows=[workflow], total_duration=45.0)

    def test_ai_format_with_complex_failures(self, complex_execution_result: ExecutionResult):
        """AIå½¢å¼ã§ã®è¤‡é›‘ãªå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ•´å½¢ãƒ†ã‚¹ãƒˆ"""
        formatter_manager = get_formatter_manager()

        output = formatter_manager.format_log(complex_execution_result, "ai")

        # AIå½¢å¼ã®å¿…é ˆè¦ç´ ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert "# CI Failure Report" in output or "## " in output
        assert "AssertionError" in output
        assert "ConnectionError" in output
        assert "SyntaxError" in output

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨è¡Œç•ªå·ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert "test_api.py" in output
        assert "connection.py" in output
        assert "parser.py" in output

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert "def test_api_endpoint" in output or "test_api_endpoint" in output

    def test_human_format_with_rich_markup(self, complex_execution_result: ExecutionResult):
        """äººé–“å¯èª­å½¢å¼ã§ã®Richãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ"""
        formatter_manager = get_formatter_manager()

        output = formatter_manager.format_log(complex_execution_result, "human")

        # Rich ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert "[" in output and "]" in output

        # è‰²ä»˜ã‘ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ã®ä¾‹ - ã‚ˆã‚Šå…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        rich_patterns = ["[red]", "[green]", "[yellow]", "[cyan]", "[bold]", "[dim]", "[/", "style="]
        has_rich_markup = any(pattern in output for pattern in rich_patterns)

        # Rich ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€å‡ºåŠ›å†…å®¹ã‚’ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
        if not has_rich_markup:
            print(f"Debug: Output content (first 500 chars): {output[:500]}")
            # ã‚ˆã‚Šç·©ã„æ¡ä»¶ã§ãƒã‚§ãƒƒã‚¯ - Rich Consoleå‡ºåŠ›ã®ç‰¹å¾´çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            console_patterns = ["âœ…", "âŒ", "ğŸ¯", "ğŸ“‹", "ğŸš¨", "CIå®Ÿè¡Œçµæœ", "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼", "å¤±æ•—"]
            has_console_output = any(pattern in output for pattern in console_patterns)
            assert has_console_output, f"Rich Consoleå‡ºåŠ›ã®ç‰¹å¾´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡ºåŠ›: {output[:200]}..."

    def test_json_format_structure_validation(self, complex_execution_result: ExecutionResult):
        """JSONå½¢å¼ã®æ§‹é€ æ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        formatter_manager = get_formatter_manager()

        output = formatter_manager.format_log(complex_execution_result, "json")

        # æœ‰åŠ¹ãªJSONã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        try:
            data = json.loads(output)
        except json.JSONDecodeError as e:
            pytest.fail(f"ç„¡åŠ¹ãªJSONå‡ºåŠ›: {e}")

        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
        assert "execution_summary" in data
        assert "workflows" in data
        assert "all_failures" in data

        # execution_summaryå†…ã®å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ç¢ºèª
        assert "success" in data["execution_summary"]
        assert "total_duration" in data["execution_summary"]

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ç¢ºèª
        assert isinstance(data["workflows"], list)
        assert len(data["workflows"]) == 2

        # å¤±æ•—æƒ…å ±ã®æ§‹é€ ç¢ºèª
        workflow = data["workflows"][0]
        assert "jobs" in workflow
        assert isinstance(workflow["jobs"], list)

        job = workflow["jobs"][0]
        assert "failures" in job
        assert isinstance(job["failures"], list)

        if job["failures"]:
            failure = job["failures"][0]
            assert "type" in failure
            assert "message" in failure

    def test_empty_result_formatting(self, empty_execution_result: ExecutionResult):
        """ç©ºã®çµæœï¼ˆå¤±æ•—ãªã—ï¼‰ã®æ•´å½¢ãƒ†ã‚¹ãƒˆ"""
        formatter_manager = get_formatter_manager()

        # å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ç©ºã®çµæœã‚’å‡¦ç†
        ai_output = formatter_manager.format_log(empty_execution_result, "ai")
        human_output = formatter_manager.format_log(empty_execution_result, "human")
        json_output = formatter_manager.format_log(empty_execution_result, "json")

        # å‡ºåŠ›ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert ai_output.strip()
        assert human_output.strip()
        assert json_output.strip()

        # æˆåŠŸçŠ¶æ…‹ãŒåæ˜ ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert "æˆåŠŸ" in ai_output or "Success" in ai_output or "âœ“" in ai_output

        # JSONã®æˆåŠŸãƒ•ãƒ©ã‚°ç¢ºèª
        json_data = json.loads(json_output)
        assert json_data["execution_summary"]["success"] is True

    @patch("ci_helper.commands.format_logs._get_execution_result")
    def test_command_line_with_filter_options(self, mock_get_result: Mock, complex_execution_result: ExecutionResult):
        """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        mock_get_result.return_value = complex_execution_result

        runner = CliRunner()

        with patch("ci_helper.commands.format_logs.get_formatter_manager") as mock_manager:
            mock_formatter_manager = Mock()
            mock_formatter_manager.format_log.return_value = "Filtered output"
            mock_manager.return_value = mock_formatter_manager

            with patch("ci_helper.commands.format_logs.get_progress_manager") as mock_progress:
                mock_progress_manager = Mock()

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®execute_with_progressãŒå‘¼ã³å‡ºã•ã‚ŒãŸã¨ãã«
                # å®Ÿéš›ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’å‘¼ã³å‡ºã™ã‚ˆã†ã«è¨­å®š
                def mock_execute_with_progress(task_func, **kwargs):
                    return task_func()  # task_funcã‚’å®Ÿè¡Œã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’å‘¼ã³å‡ºã™

                mock_progress_manager.execute_with_progress.side_effect = mock_execute_with_progress
                mock_progress.return_value = mock_progress_manager

                # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãã§å®Ÿè¡Œ
                result = runner.invoke(
                    cli,
                    ["format-logs", "--format", "ai", "--filter-errors", "--verbose-level", "detailed"],
                )

                assert result.exit_code == 0

                # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ãŒæ­£ã—ã„ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å‘¼ã³å‡ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
                call_args = mock_formatter_manager.format_log.call_args
                if call_args and len(call_args) > 1:
                    assert call_args[1]["filter_errors"] is True
                    assert call_args[1]["verbose_level"] == "detailed"
                else:
                    # call_argsãŒNoneã¾ãŸã¯ä¸æ­£ãªå ´åˆã¯ã€å°‘ãªãã¨ã‚‚å‘¼ã³å‡ºã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
                    mock_formatter_manager.format_log.assert_called()

    def test_format_consistency_with_different_options(self, complex_execution_result: ExecutionResult):
        """ç•°ãªã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        formatter_manager = get_formatter_manager()

        # åŸºæœ¬ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        basic_output = formatter_manager.format_log(
            complex_execution_result,
            "ai",
            filter_errors=False,
            verbose_level="normal",
        )

        # è©³ç´°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        detailed_output = formatter_manager.format_log(
            complex_execution_result,
            "ai",
            filter_errors=False,
            verbose_level="detailed",
        )

        # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        filtered_output = formatter_manager.format_log(
            complex_execution_result,
            "ai",
            filter_errors=True,
            verbose_level="normal",
        )

        # å„å‡ºåŠ›ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert basic_output.strip()
        assert detailed_output.strip()
        assert filtered_output.strip()

        # è©³ç´°ãƒ¬ãƒ™ãƒ«ã«ã‚ˆã£ã¦å‡ºåŠ›é‡ãŒå¤‰ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        # ï¼ˆè©³ç´°ç‰ˆã®æ–¹ãŒé•·ã„ã€ã¾ãŸã¯ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’å«ã‚€ï¼‰
        assert len(detailed_output) >= len(basic_output) or "è©³ç´°" in detailed_output

    @patch("ci_helper.commands.format_logs._get_execution_result")
    def test_file_output_integration(self, mock_get_result: Mock, complex_execution_result: ExecutionResult):
        """ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›çµ±åˆãƒ†ã‚¹ãƒˆ"""
        mock_get_result.return_value = complex_execution_result

        runner = CliRunner()

        with tempfile.TemporaryDirectory() as temp_dir:
            output_file = Path(temp_dir) / "formatted_output.md"

            with patch("ci_helper.commands.format_logs.get_formatter_manager") as mock_manager:
                mock_formatter_manager = Mock()
                mock_formatter_manager.format_log.return_value = "# Formatted Content\n\nTest output"
                mock_manager.return_value = mock_formatter_manager

                with patch("ci_helper.commands.format_logs.get_progress_manager") as mock_progress:
                    mock_progress_manager = Mock()
                    mock_progress_manager.execute_with_progress.return_value = "# Formatted Content\n\nTest output"
                    mock_progress.return_value = mock_progress_manager

                    with patch("ci_helper.utils.file_save_utils.FileSaveManager.save_formatted_log") as mock_save:
                        mock_save.return_value = (True, str(output_file))

                        result = runner.invoke(
                            cli,
                            ["format-logs", "--format", "ai", "--output", str(output_file), "--no-confirm"],
                        )

                        assert result.exit_code == 0
                        mock_save.assert_called_once()

                        # ä¿å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¢ºèª
                        call_args = mock_save.call_args
                        assert call_args[1]["content"] == "# Formatted Content\n\nTest output"
                        assert call_args[1]["format_type"] == "ai"

    def test_error_handling_with_invalid_log_data(self):
        """ç„¡åŠ¹ãªãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã§ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        formatter_manager = get_formatter_manager()

        # ä¸æ­£ãªå®Ÿè¡Œçµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        invalid_result = ExecutionResult(
            success=None,  # ä¸æ­£ãªå€¤
            workflows=None,  # ä¸æ­£ãªå€¤
            total_duration=-1.0,  # ä¸æ­£ãªå€¤
        )

        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ãŒã‚¨ãƒ©ãƒ¼ã‚’é©åˆ‡ã«å‡¦ç†ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        try:
            output = formatter_manager.format_log(invalid_result, "ai")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„å ´åˆã¯ã€ä½•ã‚‰ã‹ã®å‡ºåŠ›ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
            assert output is not None
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆã¯ã€é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            # UserInputErrorã‚‚è¨±å¯ã™ã‚‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒæŠ•ã’ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
            from ci_helper.core.exceptions import UserInputError

            assert isinstance(e, ValueError | TypeError | AttributeError | UserInputError)

    @patch("rich.prompt.Prompt.ask")
    def test_menu_navigation_with_back_operations(self, mock_prompt: Mock):
        """ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®æˆ»ã‚‹æ“ä½œãƒ†ã‚¹ãƒˆ"""
        from ci_helper.ui.command_menus import CommandMenuBuilder
        from ci_helper.ui.menu_system import MenuSystem

        console = Console()
        command_handlers = {
            "format_logs": Mock(return_value=True),
        }

        builder = CommandMenuBuilder(console, command_handlers)
        menu_system = MenuSystem(console)

        # æ·±ã„ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã¨æˆ»ã‚‹æ“ä½œ
        # ãƒ­ã‚°ç®¡ç† â†’ ãƒ­ã‚°æ•´å½¢ â†’ æœ€æ–°ãƒ­ã‚°æ•´å½¢ â†’ AIå½¢å¼ â†’ æˆ»ã‚‹ â†’ æˆ»ã‚‹ â†’ æˆ»ã‚‹ â†’ çµ‚äº†
        # å®‰å®šã—ãŸãƒ¢ãƒƒã‚¯è¨­å®šã‚’ä½¿ç”¨ã—ã¦StopIterationã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
        setup_stable_prompt_mock(mock_prompt, ["5", "4", "1", "1", "", "b", "b", "b", "b", "q"])

        main_menu = builder.build_main_menu()

        with patch.object(menu_system, "console") as mock_console:
            mock_console.clear = Mock()
            mock_console.print = Mock()

            # ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            menu_system.run_menu(main_menu)

        # ã‚³ãƒãƒ³ãƒ‰ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        command_handlers["format_logs"].assert_called_once()

    def test_performance_with_large_failure_set(self):
        """å¤§é‡ã®å¤±æ•—ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        import time

        # å¤§é‡ã®å¤±æ•—ã‚’å«ã‚€å®Ÿè¡Œçµæœã‚’ä½œæˆ
        failures = []
        for i in range(100):  # 100å€‹ã®å¤±æ•—
            failures.append(
                Failure(
                    type=FailureType.ERROR,
                    message=f"Error {i}: Test failure message",
                    file_path=f"test_file_{i}.py",
                    line_number=i + 1,
                    context_before=[f"line {i}", f"line {i + 1}"],
                    context_after=[f"line {i + 3}", f"line {i + 4}"],
                    stack_trace=f"Traceback for error {i}",
                ),
            )

        job = JobResult(name="test", success=False, failures=failures, duration=300.0)
        workflow = WorkflowResult(name="test.yml", success=False, jobs=[job], duration=300.0)
        large_result = ExecutionResult(success=False, workflows=[workflow], total_duration=300.0)

        formatter_manager = get_formatter_manager()

        # å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã®å‡¦ç†æ™‚é–“ã‚’æ¸¬å®š
        formats = ["ai", "human", "json"]

        for format_type in formats:
            start_time = time.time()
            output = formatter_manager.format_log(large_result, format_type)
            end_time = time.time()

            processing_time = end_time - start_time

            # å‡ºåŠ›ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
            assert output.strip()

            # å‡¦ç†æ™‚é–“ãŒåˆç†çš„ãªç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆ10ç§’ä»¥å†…ï¼‰
            assert processing_time < 10.0, f"{format_type}ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å‡¦ç†æ™‚é–“ãŒé•·ã™ãã¾ã™: {processing_time}ç§’"

    def test_concurrent_formatting_safety(self, complex_execution_result: ExecutionResult):
        """ä¸¦è¡Œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã®å®‰å…¨æ€§ãƒ†ã‚¹ãƒˆ"""
        import threading

        formatter_manager = get_formatter_manager()
        results = []
        errors = []

        def format_task(format_type: str):
            try:
                output = formatter_manager.format_log(complex_execution_result, format_type)
                results.append((format_type, output))
            except Exception as e:
                errors.append((format_type, e))

        # è¤‡æ•°ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§åŒæ™‚ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
        threads = []
        for format_type in ["ai", "human", "json"]:
            thread = threading.Thread(target=format_task, args=(format_type,))
            threads.append(thread)
            thread.start()

        # ã™ã¹ã¦ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã®å®Œäº†ã‚’å¾…æ©Ÿ
        for thread in threads:
            thread.join()

        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
        assert not errors, f"ä¸¦è¡Œå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {errors}"

        # ã™ã¹ã¦ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§çµæœãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert len(results) == 3

        # å„çµæœãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        for format_type, output in results:
            assert output.strip(), f"{format_type}ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å‡ºåŠ›ãŒç©ºã§ã™"
